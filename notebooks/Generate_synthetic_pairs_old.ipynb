{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import datetime\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.util import ngrams\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:31:12.898997\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (0,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# Generate features for dataset\n",
    "df = pd.concat([pd.read_csv(f, delimiter='\\t') for f in glob.glob('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/data/user-ct-test-collection-*.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled history dataset (for suffixes)\n",
    "This sample dataset is put on 1.000.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_hist = df.sample(1000000, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save samples in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_hist.to_pickle('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/sample_hist_1m.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_hist = pd.read_pickle('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/sample_hist_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create suffixes\n",
    "Create all possible suffixes, splitting per word iteratively, removing leading words.\n",
    "\n",
    "Query \"How to cook CHICKEN\" becomes:\n",
    "- how to cook chicken\n",
    "- to cook chicken\n",
    "- cook chicken\n",
    "- chicken\n",
    "\n",
    "Creating 4 suffixes.\n",
    "\n",
    "All symbols are removed and changed to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = []\n",
    "\n",
    "for row in samples_hist.itertuples():\n",
    "    line = re.sub(r\"[^A-Za-z0-9]+\", \" \", str(row.Query)).lower()\n",
    "    words = line.split()\n",
    "    for j in range(0, len(words)):\n",
    "        suffix = \" \".join(words[j:])\n",
    "        suffixes.append(suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save samples in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/hist_suffixes.pickle_1m', 'wb') as f:\n",
    "    pickle.dump(suffixes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/hist_suffixes.pickle_1m', 'rb') as f:\n",
    "    suffixes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled dataset (for prefixes)\n",
    "This sample dataset is put on 10.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data = df.sample(10000, random_state=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create prefixes\n",
    "Create all possible prefixes, splitting per letter (from second word) iteratively, removing leading words.\n",
    "\n",
    "Query \"How to cook CHICKEN\" becomes (_ = space):\n",
    "- how\n",
    "- how_\n",
    "- how_t\n",
    "- ...\n",
    "- how_to_cook_chicken\n",
    "\n",
    "Creating 17 prefixes.\n",
    "\n",
    "All symbols are removed and changed to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_list = []\n",
    "\n",
    "qid = 1\n",
    "\n",
    "for row in samples_data.itertuples():\n",
    "    line = re.sub(r\"[^A-Za-z0-9]+\", \" \", str(row.Query)).lower()\n",
    "    \n",
    "    # Ignore empty strings\n",
    "    if len(line.split()) > 1:   \n",
    "        firstword = line.split()[0]\n",
    "    \n",
    "        for j in range(len(firstword) + 2 , len(line)+1):\n",
    "            if (len(line[0:j].split())) <= 0:\n",
    "                print(line)\n",
    "                print('m' + str(line[0:j]) + 'm')\n",
    "            # temp list will be filled as [0]: 'clean_query', [1]: 'qid', [2]: 'prefix'\n",
    "            temp_list = ['', '', '']\n",
    "            temp_list[0] = line\n",
    "            temp_list[1] = \"qid:\" + str(qid)\n",
    "            temp_list[2] = line[0:j]\n",
    "            # Add to prefix query list\n",
    "            prefix_list.append(temp_list)\n",
    "            qid += 1\n",
    "    \n",
    "prefix_queries = pd.DataFrame.from_records(prefix_list)\n",
    "prefix_queries.columns = ['Query_clean', 'Qid', 'Prefix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save samples in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_queries.to_pickle('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/prefixes_1m.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_queries = pd.read_pickle('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/prefixes_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic queries\n",
    "Combine end term of prefix with top 10 suffixes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_dict = collections.Counter(suffixes)\n",
    "suffix_list = suffix_dict.most_common()\n",
    "suff_set_sorted = sorted([i[0] for i in suffix_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee1204b97184420876e5f1d146ef6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=107409.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6ad5e151babc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtemp_suffix_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuffix_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtemp_suffix_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_suffix_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if it contains suffix\n",
    "def bisect_contains_check(suffix_list, prefix):\n",
    "    try:\n",
    "        return suffix_list[bisect_left(suffix_list, prefix)].startswith(prefix)\n",
    "    except IndexError:\n",
    "        return False\n",
    "\n",
    "# Returns the prefix keys\n",
    "def bisect_list_slice(suffix_list, prefix):\n",
    "    return suffix_list[bisect_left(suffix_list, prefix):\n",
    "         bisect_left(suffix_list, prefix[:-1] + chr(ord(prefix[-1])+1))]\n",
    "\n",
    "candidate_list = []\n",
    "\n",
    "for row in tqdm(prefix_queries.itertuples(), total=prefix_queries.shape[0]):\n",
    "    words = row.Prefix.split()\n",
    "    endterm = words[-1]\n",
    "    no_endterm = \" \".join(words[:-1])\n",
    "    \n",
    "    if (bisect_contains_check(suff_set_sorted, endterm)):\n",
    "        temp_keys = bisect_list_slice(suff_set_sorted, endterm)\n",
    "        \n",
    "        temp_suffix_dict = Counter()\n",
    "        \n",
    "        for key in temp_keys:\n",
    "            temp_suffix_dict[key] = suffix_dict.get(key)\n",
    "            \n",
    "        temp_suffix_list = temp_suffix_dict.most_common()[:10]\n",
    "        \n",
    "        for j in temp_suffix_list: \n",
    "            # Last four will be filled as [5]: 'suffix', [6]: 'Hist_Suffix_freq', [7]: 'Synthetic_query' [8]: 'matching'\n",
    "            temp_list = [row.Query_clean, row.Qid, row.Prefix, '', '', '', '']\n",
    "            temp_list[3] = j[0]\n",
    "            temp_list[4] = j[1]\n",
    "            temp_list[5] = str(no_endterm + \" \" + str(j[0]))\n",
    "            temp_list[6] = 0\n",
    "            \n",
    "            if str(row.Query_clean) == str(no_endterm + \" \"+ str(j[0])):\n",
    "                temp_list[6] = 1\n",
    "            \n",
    "            # Add to synthetic query list\n",
    "            candidate_list.append(temp_list)\n",
    "    \n",
    "syn_candidate_queries = pd.DataFrame.from_records(candidate_list)\n",
    "syn_candidate_queries.columns = ['Query_clean', 'Qid', 'Prefix', 'Suffix', 'Hist_Suffix_freq', 'Synthetic_query', 'Synthetic_match']    \n",
    "    \n",
    "syn_candidate_queries.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_candidate_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = syn_candidate_queries.loc[syn_candidate_queries['Synthetic_match'] == 1]\n",
    "temp_df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save samples in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_queries.to_pickle('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/syn_candidate_queries_5m.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_queries = pd.read_pickle('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/syn_candidate_queries_5m.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suffix_dict.get('0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0')\n",
    "print(len(suff_set_sorted))\n",
    "print(len(suffixes))\n",
    "print(suff_set_sorted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "print(datetime.datetime.now().time())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
