{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk.util import ngrams\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (0,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# Generate features for dataset\n",
    "df = pd.concat([pd.read_csv(f, delimiter='\\t') for f in glob.glob('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/data/user-ct-test-collection-*.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled dataset\n",
    "This sample dataset is put on 1.000.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df.sample(1000000, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save samples in pickle file & CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_pickle('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/sample_million.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples.to_csv(r'/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/sample_dataset.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples = pd.read_pickle('test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create all prefix and suffix combinations for queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all queries shorter than 2\n",
    "def split_words(string): \n",
    "    if isinstance(string, str):\n",
    "        line = re.sub(r\"\\.(.)\", r\" \\1\", string)\n",
    "        words = line.split()\n",
    "        return len(words)\n",
    "    return(0)\n",
    "\n",
    "samples['filtered'] = samples[samples.notnull()]['Query'].map(lambda query: split_words(query))\n",
    "samples = samples[samples.filtered > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artificial_queries(samples): \n",
    "    artif_list = []\n",
    "\n",
    "    for row in samples.itertuples():\n",
    "        if isinstance(str(row.Query), str):\n",
    "            line = re.sub(r\"\\.(.)\", r\" \\1\", row.Query)\n",
    "            words = line.split()\n",
    "            # iterate #-filtered times\n",
    "            for j in range(1, len(words)):\n",
    "                # Last two will be filled as [6]: 'prefix', [7]'suffix'\n",
    "                temp_list = [row.Index, row.AnonID, row.QueryTime, row.ItemRank, row.ClickURL, row.Query, '', '']\n",
    "                prefix = \" \".join(words[:j])\n",
    "                temp_list[6] = prefix\n",
    "                suffix = \" \".join(words[j:])\n",
    "                temp_list[7] = suffix\n",
    "                # Add to artificial query list\n",
    "                artif_list.append(temp_list)   \n",
    "    return artif_list\n",
    "\n",
    "artif_list = create_artificial_queries(samples)\n",
    "queries = pd.DataFrame.from_records(artif_list)\n",
    "queries.columns = ['Index', 'AnonID', 'QueryTime', 'ItemRank', 'ClickURL', 'Query', 'Prefix', 'Suffix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create N-gram, NER & Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = []\n",
    "\n",
    "for row in queries.itertuples():\n",
    "    query = row.Query\n",
    "    if isinstance(query, str):\n",
    "        line = re.sub(r\"\\.(.)\", r\" \\1\", query)\n",
    "        words = line.split()\n",
    "        for j in range(0, len(words)):\n",
    "            suffix = \" \".join(words[j:])\n",
    "            suffixes.append(suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile(\"/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/suffixes.npy.gz\", \"w\")\n",
    "np.save(file=f, arr=suffixes)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = gzip.GzipFile('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/pickles/historical_logs.npy.gz', \"r\")\n",
    "# suffixes = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create N-gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_dict = collections.Counter(suffixes)\n",
    "\n",
    "def ngram_freq_per_n(candidate, historical_dict, n):\n",
    "    words = candidate.split()\n",
    "    ngram_n = 0\n",
    "    ngrams_i = ngrams(words, n)\n",
    "    \n",
    "    for word in ngrams_i:\n",
    "        freq_g = historical_dict[\" \".join(word)]\n",
    "        ngram_n += freq_g\n",
    "        \n",
    "    return ngram_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile(\"/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/historical_dict.npy.gz\", \"w\")\n",
    "np.save(file=f, arr=historical_dict)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_max = 6\n",
    "\n",
    "for i in range(1, ngram_max + 1):\n",
    "    ngram_name = 'ngram_' + str(i)\n",
    "    queries[ngram_name] = queries[queries.notnull()]['Query'].map(lambda candidate_row: ngram_freq_per_n(candidate_row, historical_dict, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create NER Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_contains_and_count_norm(candidate): \n",
    "    entities = nlp(candidate)\n",
    "    contains = 0\n",
    "    ner_norm = 0\n",
    "    \n",
    "    if len(entities) > 0:\n",
    "        contains = 1\n",
    "        ner_norm = len(entities) / len(candidate)\n",
    "    \n",
    "    return [contains, ner_norm]\n",
    "\n",
    "queries['has_ne'], queries['ne_norm'] = zip(*queries[queries.notnull()]['Query'].map(lambda candidate_row: ner_contains_and_count_norm(candidate_row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create \"Other\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_features(prefix, suffix, historical_dict):     \n",
    "    # The complete query\n",
    "    complete = \"\"\n",
    "    if bool_space:\n",
    "        complete = prefix + suffix\n",
    "    else:\n",
    "        complete = prefix + \" \" + suffix\n",
    "    \n",
    "    # The frequency of the candidate in the historical logs\n",
    "    frequency = historical_dict[complete]\n",
    "    \n",
    "    # Prefix, suffix and total length in characters\n",
    "    prefixlen_char = len(prefix)\n",
    "    suffixlen_char = len(suffix)\n",
    "    totallen_char = len(complete)\n",
    "    \n",
    "    # Prefix, suffix and total length in words\n",
    "    prefixlen_word = len(prefix.split())\n",
    "    suffixlen_word = len(suffix.split())\n",
    "    totallen_word = len(complete.split())\n",
    "    \n",
    "    return [frequency, \n",
    "            prefixlen_char, suffixlen_char, totallen_char,\n",
    "            prefixlen_word, suffixlen_word, totallen_word]\n",
    "\n",
    "queries['candid_freq'], queries['prefixlen_char'], queries['suffixlen_char'], queries['totallen_char'], queries['prefixlen_word'], queries['suffixlen_word'], queries['totallen_word'] = zip(*queries.apply(lambda query_row: get_other_features(query_row.Prefix, query_row.Suffix, historical_dict), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.to_pickle(r'/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/artif_queries_dataset_big.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
