{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yusuf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "Using TensorFlow backend.\n",
      "c:\\users\\yusuf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm_notebook\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matchzoo as mz\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('data/data-part-01.txt', newline = '') as aol_data:                                                                                          \n",
    "    data = csv.reader(aol_data, delimiter='\\t')\n",
    "    data = list(data)\n",
    "    \n",
    "df = pd.DataFrame.from_records(data[1:], columns=data[0])\n",
    "df['ItemRank'] = pd.to_numeric(df['ItemRank'], errors='ignore')\n",
    "# queries = df['Query'].tolist()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb702202eb642989c56ddbda294398c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.sample(10000, random_state=23)\n",
    "\n",
    "for index, row in tqdm_notebook(df.iterrows()):\n",
    "    if row['ItemRank'] >= 0:\n",
    "        df.loc[index, 'Clicked'] = 1\n",
    "    else:\n",
    "        df.loc[index, 'Clicked'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = df['Query'].tolist()\n",
    "clicked = df['Clicked'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = []\n",
    "suffixes = []\n",
    "pairs = []\n",
    "labels = []\n",
    "\n",
    "for i in range(5000):\n",
    "    words = queries[i].split()\n",
    "    for j in range(1, len(words)):\n",
    "        prefix = \" \".join(words[:j])\n",
    "        suffix = \" \".join(words[j:])\n",
    "        prefixes.append(prefix)\n",
    "        suffixes.append(suffix)\n",
    "        pairs.append([prefix, suffix])\n",
    "        labels.append(clicked[i])\n",
    "\n",
    "del queries\n",
    "del clicked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(original_list, weight_list):\n",
    "    sublists = []\n",
    "    prev_index = 0\n",
    "    for weight in weight_list:\n",
    "        next_index = prev_index + math.ceil( (len(original_list) * weight) )\n",
    "\n",
    "        sublists.append( original_list[prev_index : next_index] )\n",
    "        prev_index = next_index\n",
    "\n",
    "    return sublists\n",
    "\n",
    "tr_prefix, val_prefix, test_prefix = split(prefixes, [0.7, 0.2, 0.1])\n",
    "tr_suffix, val_suffix, test_suffix = split(suffixes, [0.7, 0.2, 0.1])\n",
    "tr_labels, val_labels, test_labels = split(labels, [0.7, 0.2, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>text_left</th>\n",
       "      <th>id_right</th>\n",
       "      <th>text_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L-0</td>\n",
       "      <td>social</td>\n",
       "      <td>R-0</td>\n",
       "      <td>work malpractice insurance</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L-1</td>\n",
       "      <td>social work</td>\n",
       "      <td>R-1</td>\n",
       "      <td>malpractice insurance</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L-2</td>\n",
       "      <td>social work malpractice</td>\n",
       "      <td>R-2</td>\n",
       "      <td>insurance</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L-3</td>\n",
       "      <td>northern</td>\n",
       "      <td>R-3</td>\n",
       "      <td>tool</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L-4</td>\n",
       "      <td>colt</td>\n",
       "      <td>R-4</td>\n",
       "      <td>government 380 handgun grips</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>L-706</td>\n",
       "      <td>vintage ornateserving fork brass nude</td>\n",
       "      <td>R-705</td>\n",
       "      <td>female and cherub bordini italy</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>L-707</td>\n",
       "      <td>vintage ornateserving fork brass nude female</td>\n",
       "      <td>R-706</td>\n",
       "      <td>and cherub bordini italy</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>L-708</td>\n",
       "      <td>vintage ornateserving fork brass nude female and</td>\n",
       "      <td>R-707</td>\n",
       "      <td>cherub bordini italy</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>L-709</td>\n",
       "      <td>vintage ornateserving fork brass nude female a...</td>\n",
       "      <td>R-708</td>\n",
       "      <td>bordini italy</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>L-710</td>\n",
       "      <td>vintage ornateserving fork brass nude female a...</td>\n",
       "      <td>R-19</td>\n",
       "      <td>italy</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>736 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_left                                          text_left id_right  \\\n",
       "0       L-0                                             social      R-0   \n",
       "1       L-1                                        social work      R-1   \n",
       "2       L-2                            social work malpractice      R-2   \n",
       "3       L-3                                           northern      R-3   \n",
       "4       L-4                                               colt      R-4   \n",
       "..      ...                                                ...      ...   \n",
       "731   L-706              vintage ornateserving fork brass nude    R-705   \n",
       "732   L-707       vintage ornateserving fork brass nude female    R-706   \n",
       "733   L-708   vintage ornateserving fork brass nude female and    R-707   \n",
       "734   L-709  vintage ornateserving fork brass nude female a...    R-708   \n",
       "735   L-710  vintage ornateserving fork brass nude female a...     R-19   \n",
       "\n",
       "                          text_right  label  \n",
       "0         work malpractice insurance    0.0  \n",
       "1              malpractice insurance    0.0  \n",
       "2                          insurance    0.0  \n",
       "3                               tool    0.0  \n",
       "4       government 380 handgun grips    1.0  \n",
       "..                               ...    ...  \n",
       "731  female and cherub bordini italy    0.0  \n",
       "732         and cherub bordini italy    0.0  \n",
       "733             cherub bordini italy    0.0  \n",
       "734                    bordini italy    0.0  \n",
       "735                            italy    0.0  \n",
       "\n",
       "[736 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame({\n",
    "    'text_left': tr_prefix,\n",
    "    'text_right': tr_suffix,\n",
    "    'label': tr_labels\n",
    "})\n",
    "\n",
    "val_data = pd.DataFrame({\n",
    "    'text_left': val_prefix,\n",
    "    'text_right': val_suffix,\n",
    "    'label': val_labels\n",
    "})\n",
    "\n",
    "test_data = pd.DataFrame({\n",
    "    'text_left': test_prefix,\n",
    "    'text_right': test_suffix,\n",
    "    'label': test_labels\n",
    "})\n",
    "\n",
    "train_pack = mz.pack(train_data)\n",
    "val_pack = mz.pack(val_data)\n",
    "test_pack = mz.pack(test_data)\n",
    "test_pack.frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval => NgramLetter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4518/4518 [00:00<00:00, 12859.79it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval => NgramLetter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4487/4487 [00:00<00:00, 12956.09it/s]\n",
      "Processing text_left with extend: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4518/4518 [00:00<00:00, 904355.52it/s]\n",
      "Processing text_right with extend: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4487/4487 [00:00<00:00, 896652.63it/s]\n",
      "Building Vocabulary from a datapack.: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 105362/105362 [00:00<00:00, 3508863.13it/s]\n",
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4518/4518 [00:00<00:00, 14105.92it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4487/4487 [00:00<00:00, 14186.47it/s]\n",
      "Processing text_left with transform: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4518/4518 [00:00<00:00, 145612.50it/s]\n",
      "Processing text_right with transform: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4487/4487 [00:00<00:00, 149433.80it/s]\n",
      "Processing text_left with chain_transform of NgramLetter => WordHashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4518/4518 [00:04<00:00, 954.92it/s] \n",
      "Processing text_right with chain_transform of NgramLetter => WordHashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4487/4487 [00:07<00:00, 586.85it/s]\n",
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1381/1381 [00:00<00:00, 10779.26it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1403/1403 [00:00<00:00, 13100.14it/s]\n",
      "Processing text_left with transform: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1381/1381 [00:00<00:00, 106131.41it/s]\n",
      "Processing text_right with transform: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1403/1403 [00:00<00:00, 116804.46it/s]\n",
      "Processing text_left with chain_transform of NgramLetter => WordHashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1381/1381 [00:02<00:00, 546.87it/s]\n",
      "Processing text_right with chain_transform of NgramLetter => WordHashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1403/1403 [00:02<00:00, 479.06it/s]\n",
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 711/711 [00:00<00:00, 12462.44it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 709/709 [00:00<00:00, 12879.23it/s]\n",
      "Processing text_left with transform: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 711/711 [00:00<00:00, 50737.55it/s]\n",
      "Processing text_right with transform: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 709/709 [00:00<00:00, 101206.87it/s]\n",
      "Processing text_left with chain_transform of NgramLetter => WordHashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 711/711 [01:51<00:00,  6.37it/s] \n",
      "Processing text_right with chain_transform of NgramLetter => WordHashing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 709/709 [00:01<00:00, 683.08it/s] \n"
     ]
    }
   ],
   "source": [
    "preprocessor = mz.preprocessors.CDSSMPreprocessor(fixed_length_left=10, fixed_length_right=10)\n",
    "train_pack_processed = preprocessor.fit_transform(train_pack)\n",
    "valid_pack_processed = preprocessor.transform(val_pack)\n",
    "test_pack_processed = preprocessor.transform(test_pack)\n",
    "\n",
    "ranking_task = mz.tasks.Ranking(loss=mz.losses.RankHingeLoss())\n",
    "ranking_task.metrics = [\n",
    "    mz.metrics.NormalizedDiscountedCumulativeGain(k=3),\n",
    "    mz.metrics.NormalizedDiscountedCumulativeGain(k=5),\n",
    "    mz.metrics.MeanAveragePrecision()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text_left (InputLayer)          (None, 10, 4367)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_right (InputLayer)         (None, 10, 4367)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 10, 64)       838528      text_left[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 10, 64)       838528      text_right[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 10, 64)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10, 64)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 64)           0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           4160        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            2           dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,693,698\n",
      "Trainable params: 1,693,698\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = mz.models.CDSSM()\n",
    "model.params['input_shapes'] = preprocessor.context['input_shapes']\n",
    "model.params['task'] = ranking_task\n",
    "model.params['filters'] = 64\n",
    "model.params['kernel_size'] = 3\n",
    "model.params['strides'] = 1\n",
    "model.params['padding'] = 'same'\n",
    "model.params['conv_activation_func'] = 'tanh'\n",
    "model.params['w_initializer'] = 'glorot_normal'\n",
    "model.params['b_initializer'] = 'zeros'\n",
    "model.params['mlp_num_layers'] = 1\n",
    "model.params['mlp_num_units'] = 64\n",
    "model.params['mlp_num_fan_out'] = 64\n",
    "model.params['mlp_activation_func'] = 'tanh'\n",
    "model.params['dropout_rate'] = 0.8\n",
    "model.params['optimizer'] = 'adadelta'\n",
    "model.guess_and_fill_missing_params()\n",
    "model.build()\n",
    "model.compile()\n",
    "model.backend.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num batches: 35\n"
     ]
    }
   ],
   "source": [
    "pred_x, pred_y = test_pack_processed[:].unpack()\n",
    "evaluate = mz.callbacks.EvaluateAllMetrics(model, x=pred_x, y=pred_y, batch_size=len(pred_x))\n",
    "train_generator = mz.DataGenerator(\n",
    "    train_pack_processed,\n",
    "    mode='pair',\n",
    "    num_dup=2,\n",
    "    num_neg=1,\n",
    "    batch_size=20\n",
    ")\n",
    "print('num batches:', len(train_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 1/35 [..............................] - ETA: 30s - loss: 1.0107"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yusuf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.878800). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2/35 [>.............................] - ETA: 31s - loss: 1.0223"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yusuf\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.440401). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - ETA: 22s - loss: 1.02 - ETA: 17s - loss: 1.01 - ETA: 14s - loss: 1.00 - ETA: 12s - loss: 1.00 - ETA: 10s - loss: 1.00 - ETA: 9s - loss: 1.0003 - ETA: 8s - loss: 0.996 - ETA: 7s - loss: 0.995 - ETA: 7s - loss: 0.994 - ETA: 6s - loss: 0.989 - ETA: 6s - loss: 0.991 - ETA: 5s - loss: 0.989 - ETA: 5s - loss: 0.986 - ETA: 5s - loss: 0.982 - ETA: 4s - loss: 0.983 - ETA: 4s - loss: 0.981 - ETA: 3s - loss: 0.978 - ETA: 3s - loss: 0.976 - ETA: 3s - loss: 0.967 - ETA: 3s - loss: 0.964 - ETA: 2s - loss: 0.963 - ETA: 2s - loss: 0.956 - ETA: 2s - loss: 0.951 - ETA: 1s - loss: 0.948 - ETA: 1s - loss: 0.944 - ETA: 1s - loss: 0.945 - ETA: 1s - loss: 0.944 - ETA: 0s - loss: 0.941 - ETA: 0s - loss: 0.935 - ETA: 0s - loss: 0.935 - ETA: 0s - loss: 0.929 - ETA: 0s - loss: 0.924 - 6s 180ms/step - loss: 0.9221\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6180904029062325 - normalized_discounted_cumulative_gain@5(0.0): 0.618673133586234 - mean_average_precision(0.0): 0.6179827205143661\n",
      "Epoch 2/20\n",
      "35/35 [==============================] - ETA: 5s - loss: 0.623 - ETA: 5s - loss: 0.590 - ETA: 6s - loss: 0.627 - ETA: 5s - loss: 0.631 - ETA: 5s - loss: 0.649 - ETA: 5s - loss: 0.642 - ETA: 5s - loss: 0.656 - ETA: 5s - loss: 0.659 - ETA: 4s - loss: 0.630 - ETA: 4s - loss: 0.619 - ETA: 4s - loss: 0.626 - ETA: 4s - loss: 0.634 - ETA: 3s - loss: 0.621 - ETA: 3s - loss: 0.616 - ETA: 3s - loss: 0.623 - ETA: 3s - loss: 0.616 - ETA: 3s - loss: 0.612 - ETA: 3s - loss: 0.613 - ETA: 2s - loss: 0.606 - ETA: 2s - loss: 0.594 - ETA: 2s - loss: 0.594 - ETA: 2s - loss: 0.594 - ETA: 2s - loss: 0.597 - ETA: 1s - loss: 0.601 - ETA: 1s - loss: 0.615 - ETA: 1s - loss: 0.615 - ETA: 1s - loss: 0.613 - ETA: 1s - loss: 0.609 - ETA: 0s - loss: 0.608 - ETA: 0s - loss: 0.606 - ETA: 0s - loss: 0.604 - ETA: 0s - loss: 0.601 - ETA: 0s - loss: 0.598 - ETA: 0s - loss: 0.599 - 5s 139ms/step - loss: 0.6028\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6190259193020052 - normalized_discounted_cumulative_gain@5(0.0): 0.6193086489640831 - mean_average_precision(0.0): 0.6187559998214007\n",
      "Epoch 3/20\n",
      "35/35 [==============================] - ETA: 4s - loss: 0.508 - ETA: 5s - loss: 0.498 - ETA: 4s - loss: 0.440 - ETA: 4s - loss: 0.416 - ETA: 4s - loss: 0.418 - ETA: 4s - loss: 0.428 - ETA: 4s - loss: 0.430 - ETA: 3s - loss: 0.444 - ETA: 3s - loss: 0.455 - ETA: 3s - loss: 0.462 - ETA: 3s - loss: 0.466 - ETA: 3s - loss: 0.457 - ETA: 3s - loss: 0.449 - ETA: 3s - loss: 0.466 - ETA: 2s - loss: 0.457 - ETA: 2s - loss: 0.463 - ETA: 2s - loss: 0.460 - ETA: 2s - loss: 0.460 - ETA: 2s - loss: 0.451 - ETA: 2s - loss: 0.440 - ETA: 2s - loss: 0.449 - ETA: 1s - loss: 0.452 - ETA: 1s - loss: 0.447 - ETA: 1s - loss: 0.441 - ETA: 1s - loss: 0.436 - ETA: 1s - loss: 0.430 - ETA: 1s - loss: 0.426 - ETA: 0s - loss: 0.423 - ETA: 0s - loss: 0.417 - ETA: 0s - loss: 0.416 - ETA: 0s - loss: 0.411 - ETA: 0s - loss: 0.407 - ETA: 0s - loss: 0.406 - ETA: 0s - loss: 0.405 - 4s 124ms/step - loss: 0.4007\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6191123363877359 - normalized_discounted_cumulative_gain@5(0.0): 0.6193950660498138 - mean_average_precision(0.0): 0.6189750630678901\n",
      "Epoch 4/20\n",
      "35/35 [==============================] - ETA: 5s - loss: 0.516 - ETA: 5s - loss: 0.419 - ETA: 5s - loss: 0.413 - ETA: 5s - loss: 0.414 - ETA: 5s - loss: 0.431 - ETA: 4s - loss: 0.417 - ETA: 4s - loss: 0.401 - ETA: 4s - loss: 0.376 - ETA: 4s - loss: 0.372 - ETA: 3s - loss: 0.358 - ETA: 3s - loss: 0.352 - ETA: 3s - loss: 0.337 - ETA: 3s - loss: 0.330 - ETA: 3s - loss: 0.326 - ETA: 3s - loss: 0.328 - ETA: 2s - loss: 0.328 - ETA: 2s - loss: 0.316 - ETA: 2s - loss: 0.325 - ETA: 2s - loss: 0.315 - ETA: 2s - loss: 0.317 - ETA: 2s - loss: 0.319 - ETA: 1s - loss: 0.319 - ETA: 1s - loss: 0.319 - ETA: 1s - loss: 0.321 - ETA: 1s - loss: 0.322 - ETA: 1s - loss: 0.319 - ETA: 1s - loss: 0.322 - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.313 - ETA: 0s - loss: 0.315 - ETA: 0s - loss: 0.322 - ETA: 0s - loss: 0.318 - ETA: 0s - loss: 0.323 - ETA: 0s - loss: 0.325 - 4s 125ms/step - loss: 0.3303\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.6194408213472973 - mean_average_precision(0.0): 0.6190922688812984\n",
      "Epoch 5/20\n",
      "35/35 [==============================] - ETA: 13s - loss: 0.18 - ETA: 9s - loss: 0.3132 - ETA: 8s - loss: 0.333 - ETA: 7s - loss: 0.292 - ETA: 6s - loss: 0.265 - ETA: 5s - loss: 0.252 - ETA: 5s - loss: 0.230 - ETA: 5s - loss: 0.231 - ETA: 5s - loss: 0.249 - ETA: 4s - loss: 0.259 - ETA: 4s - loss: 0.252 - ETA: 4s - loss: 0.261 - ETA: 4s - loss: 0.282 - ETA: 3s - loss: 0.276 - ETA: 3s - loss: 0.268 - ETA: 3s - loss: 0.267 - ETA: 3s - loss: 0.266 - ETA: 3s - loss: 0.258 - ETA: 2s - loss: 0.256 - ETA: 2s - loss: 0.258 - ETA: 2s - loss: 0.256 - ETA: 2s - loss: 0.264 - ETA: 2s - loss: 0.264 - ETA: 1s - loss: 0.258 - ETA: 1s - loss: 0.259 - ETA: 1s - loss: 0.254 - ETA: 1s - loss: 0.260 - ETA: 1s - loss: 0.265 - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.265 - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.258 - ETA: 0s - loss: 0.264 - 5s 144ms/step - loss: 0.2591\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6191123363877359 - normalized_discounted_cumulative_gain@5(0.0): 0.6193950660498138 - mean_average_precision(0.0): 0.6189345991561181\n",
      "Epoch 6/20\n",
      "35/35 [==============================] - ETA: 4s - loss: 0.356 - ETA: 5s - loss: 0.257 - ETA: 5s - loss: 0.296 - ETA: 5s - loss: 0.252 - ETA: 4s - loss: 0.214 - ETA: 4s - loss: 0.226 - ETA: 4s - loss: 0.236 - ETA: 3s - loss: 0.230 - ETA: 3s - loss: 0.232 - ETA: 3s - loss: 0.226 - ETA: 3s - loss: 0.231 - ETA: 3s - loss: 0.225 - ETA: 3s - loss: 0.218 - ETA: 3s - loss: 0.217 - ETA: 2s - loss: 0.222 - ETA: 2s - loss: 0.231 - ETA: 2s - loss: 0.231 - ETA: 2s - loss: 0.239 - ETA: 2s - loss: 0.236 - ETA: 2s - loss: 0.240 - ETA: 2s - loss: 0.234 - ETA: 1s - loss: 0.237 - ETA: 1s - loss: 0.231 - ETA: 1s - loss: 0.233 - ETA: 1s - loss: 0.229 - ETA: 1s - loss: 0.222 - ETA: 1s - loss: 0.227 - ETA: 0s - loss: 0.230 - ETA: 0s - loss: 0.233 - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.235 - ETA: 0s - loss: 0.231 - ETA: 0s - loss: 0.229 - ETA: 0s - loss: 0.228 - 4s 123ms/step - loss: 0.2250\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.619625357321295 - mean_average_precision(0.0): 0.6190830598531021\n",
      "Epoch 7/20\n",
      "35/35 [==============================] - ETA: 5s - loss: 0.112 - ETA: 5s - loss: 0.077 - ETA: 5s - loss: 0.215 - ETA: 5s - loss: 0.220 - ETA: 5s - loss: 0.193 - ETA: 4s - loss: 0.192 - ETA: 4s - loss: 0.191 - ETA: 4s - loss: 0.201 - ETA: 4s - loss: 0.216 - ETA: 4s - loss: 0.202 - ETA: 3s - loss: 0.190 - ETA: 3s - loss: 0.185 - ETA: 3s - loss: 0.188 - ETA: 3s - loss: 0.207 - ETA: 3s - loss: 0.200 - ETA: 3s - loss: 0.193 - ETA: 2s - loss: 0.193 - ETA: 2s - loss: 0.190 - ETA: 2s - loss: 0.192 - ETA: 2s - loss: 0.191 - ETA: 2s - loss: 0.191 - ETA: 2s - loss: 0.190 - ETA: 1s - loss: 0.189 - ETA: 1s - loss: 0.188 - ETA: 1s - loss: 0.185 - ETA: 1s - loss: 0.192 - ETA: 1s - loss: 0.190 - ETA: 1s - loss: 0.190 - ETA: 0s - loss: 0.193 - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.182 - ETA: 0s - loss: 0.184 - 5s 129ms/step - loss: 0.1827\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.6194199166042167 - mean_average_precision(0.0): 0.619016643225504\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - ETA: 4s - loss: 0.215 - ETA: 5s - loss: 0.271 - ETA: 4s - loss: 0.223 - ETA: 4s - loss: 0.186 - ETA: 4s - loss: 0.177 - ETA: 4s - loss: 0.189 - ETA: 4s - loss: 0.199 - ETA: 4s - loss: 0.196 - ETA: 3s - loss: 0.193 - ETA: 3s - loss: 0.185 - ETA: 3s - loss: 0.173 - ETA: 3s - loss: 0.174 - ETA: 3s - loss: 0.183 - ETA: 3s - loss: 0.171 - ETA: 3s - loss: 0.164 - ETA: 2s - loss: 0.158 - ETA: 2s - loss: 0.151 - ETA: 2s - loss: 0.148 - ETA: 2s - loss: 0.147 - ETA: 2s - loss: 0.150 - ETA: 2s - loss: 0.156 - ETA: 2s - loss: 0.156 - ETA: 1s - loss: 0.162 - ETA: 1s - loss: 0.163 - ETA: 1s - loss: 0.157 - ETA: 1s - loss: 0.153 - ETA: 1s - loss: 0.149 - ETA: 1s - loss: 0.148 - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.146 - ETA: 0s - loss: 0.145 - ETA: 0s - loss: 0.148 - ETA: 0s - loss: 0.148 - 5s 131ms/step - loss: 0.1475\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.619625357321295 - mean_average_precision(0.0): 0.6191039894626392\n",
      "Epoch 9/20\n",
      "35/35 [==============================] - ETA: 8s - loss: 0.085 - ETA: 6s - loss: 0.134 - ETA: 6s - loss: 0.159 - ETA: 5s - loss: 0.190 - ETA: 5s - loss: 0.179 - ETA: 5s - loss: 0.163 - ETA: 5s - loss: 0.158 - ETA: 5s - loss: 0.176 - ETA: 4s - loss: 0.167 - ETA: 4s - loss: 0.157 - ETA: 4s - loss: 0.150 - ETA: 4s - loss: 0.142 - ETA: 4s - loss: 0.137 - ETA: 3s - loss: 0.130 - ETA: 3s - loss: 0.133 - ETA: 3s - loss: 0.130 - ETA: 3s - loss: 0.127 - ETA: 3s - loss: 0.129 - ETA: 2s - loss: 0.125 - ETA: 2s - loss: 0.124 - ETA: 2s - loss: 0.121 - ETA: 2s - loss: 0.129 - ETA: 2s - loss: 0.127 - ETA: 1s - loss: 0.129 - ETA: 1s - loss: 0.128 - ETA: 1s - loss: 0.126 - ETA: 1s - loss: 0.128 - ETA: 1s - loss: 0.125 - ETA: 0s - loss: 0.125 - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.122 - ETA: 0s - loss: 0.118 - ETA: 0s - loss: 0.120 - 5s 142ms/step - loss: 0.1199\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.619625357321295 - mean_average_precision(0.0): 0.6191039894626392\n",
      "Epoch 10/20\n",
      "35/35 [==============================] - ETA: 5s - loss: 0.153 - ETA: 5s - loss: 0.171 - ETA: 5s - loss: 0.130 - ETA: 4s - loss: 0.131 - ETA: 4s - loss: 0.131 - ETA: 4s - loss: 0.140 - ETA: 4s - loss: 0.130 - ETA: 4s - loss: 0.136 - ETA: 4s - loss: 0.151 - ETA: 3s - loss: 0.137 - ETA: 3s - loss: 0.134 - ETA: 3s - loss: 0.134 - ETA: 3s - loss: 0.134 - ETA: 3s - loss: 0.145 - ETA: 3s - loss: 0.149 - ETA: 2s - loss: 0.152 - ETA: 2s - loss: 0.152 - ETA: 2s - loss: 0.144 - ETA: 2s - loss: 0.139 - ETA: 2s - loss: 0.136 - ETA: 2s - loss: 0.129 - ETA: 2s - loss: 0.124 - ETA: 1s - loss: 0.129 - ETA: 1s - loss: 0.129 - ETA: 1s - loss: 0.128 - ETA: 1s - loss: 0.130 - ETA: 1s - loss: 0.125 - ETA: 1s - loss: 0.122 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.117 - ETA: 0s - loss: 0.115 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.113 - ETA: 0s - loss: 0.116 - 5s 131ms/step - loss: 0.1180\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.6194408213472973 - mean_average_precision(0.0): 0.6190518049695265\n",
      "Epoch 11/20\n",
      "35/35 [==============================] - ETA: 4s - loss: 0.085 - ETA: 4s - loss: 0.057 - ETA: 4s - loss: 0.039 - ETA: 4s - loss: 0.029 - ETA: 4s - loss: 0.035 - ETA: 4s - loss: 0.031 - ETA: 3s - loss: 0.048 - ETA: 3s - loss: 0.045 - ETA: 3s - loss: 0.049 - ETA: 3s - loss: 0.069 - ETA: 3s - loss: 0.063 - ETA: 3s - loss: 0.058 - ETA: 3s - loss: 0.057 - ETA: 3s - loss: 0.060 - ETA: 2s - loss: 0.078 - ETA: 2s - loss: 0.083 - ETA: 2s - loss: 0.080 - ETA: 2s - loss: 0.085 - ETA: 2s - loss: 0.083 - ETA: 2s - loss: 0.084 - ETA: 2s - loss: 0.085 - ETA: 2s - loss: 0.083 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.086 - ETA: 1s - loss: 0.092 - ETA: 1s - loss: 0.091 - ETA: 1s - loss: 0.095 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.098 - ETA: 0s - loss: 0.099 - ETA: 0s - loss: 0.104 - 5s 135ms/step - loss: 0.1056\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.619625357321295 - mean_average_precision(0.0): 0.6191039894626392\n",
      "Epoch 12/20\n",
      "35/35 [==============================] - ETA: 9s - loss: 0.004 - ETA: 7s - loss: 0.109 - ETA: 6s - loss: 0.072 - ETA: 6s - loss: 0.076 - ETA: 7s - loss: 0.066 - ETA: 16s - loss: 0.06 - ETA: 14s - loss: 0.06 - ETA: 12s - loss: 0.06 - ETA: 11s - loss: 0.06 - ETA: 10s - loss: 0.06 - ETA: 9s - loss: 0.0664 - ETA: 8s - loss: 0.064 - ETA: 7s - loss: 0.070 - ETA: 7s - loss: 0.074 - ETA: 6s - loss: 0.077 - ETA: 6s - loss: 0.074 - ETA: 5s - loss: 0.076 - ETA: 5s - loss: 0.078 - ETA: 4s - loss: 0.079 - ETA: 4s - loss: 0.084 - ETA: 3s - loss: 0.083 - ETA: 3s - loss: 0.087 - ETA: 3s - loss: 0.087 - ETA: 2s - loss: 0.088 - ETA: 2s - loss: 0.089 - ETA: 2s - loss: 0.088 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.090 - ETA: 1s - loss: 0.091 - ETA: 1s - loss: 0.093 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.097 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.094 - 7s 207ms/step - loss: 0.0952\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.619625357321295 - mean_average_precision(0.0): 0.6191318956086889\n",
      "Epoch 13/20\n",
      "35/35 [==============================] - ETA: 5s - loss: 0.101 - ETA: 5s - loss: 0.059 - ETA: 5s - loss: 0.060 - ETA: 4s - loss: 0.054 - ETA: 4s - loss: 0.056 - ETA: 4s - loss: 0.060 - ETA: 4s - loss: 0.059 - ETA: 4s - loss: 0.069 - ETA: 4s - loss: 0.069 - ETA: 4s - loss: 0.069 - ETA: 3s - loss: 0.065 - ETA: 3s - loss: 0.065 - ETA: 3s - loss: 0.060 - ETA: 3s - loss: 0.059 - ETA: 3s - loss: 0.067 - ETA: 3s - loss: 0.074 - ETA: 3s - loss: 0.071 - ETA: 2s - loss: 0.071 - ETA: 2s - loss: 0.074 - ETA: 2s - loss: 0.079 - ETA: 2s - loss: 0.082 - ETA: 2s - loss: 0.086 - ETA: 2s - loss: 0.083 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.081 - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.083 - 5s 141ms/step - loss: 0.0822\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.619625357321295 - mean_average_precision(0.0): 0.6191318956086889\n",
      "Epoch 14/20\n",
      "35/35 [==============================] - ETA: 4s - loss: 0.076 - ETA: 4s - loss: 0.083 - ETA: 4s - loss: 0.078 - ETA: 4s - loss: 0.095 - ETA: 4s - loss: 0.088 - ETA: 4s - loss: 0.079 - ETA: 4s - loss: 0.080 - ETA: 4s - loss: 0.074 - ETA: 4s - loss: 0.081 - ETA: 3s - loss: 0.080 - ETA: 3s - loss: 0.089 - ETA: 3s - loss: 0.088 - ETA: 3s - loss: 0.081 - ETA: 3s - loss: 0.089 - ETA: 3s - loss: 0.088 - ETA: 3s - loss: 0.083 - ETA: 2s - loss: 0.084 - ETA: 2s - loss: 0.084 - ETA: 2s - loss: 0.084 - ETA: 2s - loss: 0.081 - ETA: 2s - loss: 0.078 - ETA: 2s - loss: 0.077 - ETA: 1s - loss: 0.078 - ETA: 1s - loss: 0.078 - ETA: 1s - loss: 0.075 - ETA: 1s - loss: 0.077 - ETA: 1s - loss: 0.075 - ETA: 1s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - 5s 135ms/step - loss: 0.0755\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.6194408213472973 - mean_average_precision(0.0): 0.6190727345790636\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - ETA: 7s - loss: 0.033 - ETA: 6s - loss: 0.042 - ETA: 6s - loss: 0.049 - ETA: 5s - loss: 0.044 - ETA: 5s - loss: 0.036 - ETA: 4s - loss: 0.030 - ETA: 4s - loss: 0.046 - ETA: 4s - loss: 0.062 - ETA: 4s - loss: 0.055 - ETA: 4s - loss: 0.061 - ETA: 4s - loss: 0.067 - ETA: 3s - loss: 0.063 - ETA: 3s - loss: 0.062 - ETA: 3s - loss: 0.072 - ETA: 3s - loss: 0.068 - ETA: 3s - loss: 0.075 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.074 - ETA: 2s - loss: 0.085 - ETA: 2s - loss: 0.087 - ETA: 2s - loss: 0.084 - ETA: 2s - loss: 0.090 - ETA: 1s - loss: 0.088 - ETA: 1s - loss: 0.086 - ETA: 1s - loss: 0.083 - ETA: 1s - loss: 0.087 - ETA: 1s - loss: 0.085 - ETA: 1s - loss: 0.086 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.085 - ETA: 0s - loss: 0.087 - ETA: 0s - loss: 0.086 - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.082 - 5s 138ms/step - loss: 0.0808\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.6194408213472973 - mean_average_precision(0.0): 0.6190294800526868\n",
      "Epoch 16/20\n",
      "35/35 [==============================] - ETA: 4s - loss: 0.206 - ETA: 5s - loss: 0.169 - ETA: 5s - loss: 0.135 - ETA: 4s - loss: 0.101 - ETA: 4s - loss: 0.089 - ETA: 4s - loss: 0.085 - ETA: 4s - loss: 0.078 - ETA: 4s - loss: 0.079 - ETA: 4s - loss: 0.081 - ETA: 3s - loss: 0.078 - ETA: 3s - loss: 0.079 - ETA: 3s - loss: 0.073 - ETA: 3s - loss: 0.073 - ETA: 3s - loss: 0.075 - ETA: 3s - loss: 0.070 - ETA: 2s - loss: 0.077 - ETA: 2s - loss: 0.074 - ETA: 2s - loss: 0.070 - ETA: 2s - loss: 0.071 - ETA: 2s - loss: 0.072 - ETA: 2s - loss: 0.071 - ETA: 2s - loss: 0.070 - ETA: 1s - loss: 0.071 - ETA: 1s - loss: 0.069 - ETA: 1s - loss: 0.070 - ETA: 1s - loss: 0.068 - ETA: 1s - loss: 0.070 - ETA: 1s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 5s 129ms/step - loss: 0.0704\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.619625357321295 - mean_average_precision(0.0): 0.6190830598531021\n",
      "Epoch 17/20\n",
      "35/35 [==============================] - ETA: 4s - loss: 0.060 - ETA: 4s - loss: 0.052 - ETA: 4s - loss: 0.044 - ETA: 4s - loss: 0.034 - ETA: 4s - loss: 0.042 - ETA: 4s - loss: 0.039 - ETA: 4s - loss: 0.058 - ETA: 4s - loss: 0.052 - ETA: 4s - loss: 0.052 - ETA: 4s - loss: 0.047 - ETA: 3s - loss: 0.049 - ETA: 3s - loss: 0.047 - ETA: 3s - loss: 0.058 - ETA: 3s - loss: 0.060 - ETA: 3s - loss: 0.068 - ETA: 3s - loss: 0.071 - ETA: 2s - loss: 0.074 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.075 - ETA: 2s - loss: 0.076 - ETA: 2s - loss: 0.073 - ETA: 2s - loss: 0.071 - ETA: 1s - loss: 0.068 - ETA: 1s - loss: 0.068 - ETA: 1s - loss: 0.065 - ETA: 1s - loss: 0.067 - ETA: 1s - loss: 0.066 - ETA: 1s - loss: 0.064 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 5s 131ms/step - loss: 0.0686\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.619625357321295 - mean_average_precision(0.0): 0.6190830598531021\n",
      "Epoch 18/20\n",
      "35/35 [==============================] - ETA: 4s - loss: 0.071 - ETA: 4s - loss: 0.073 - ETA: 4s - loss: 0.074 - ETA: 4s - loss: 0.084 - ETA: 4s - loss: 0.078 - ETA: 4s - loss: 0.085 - ETA: 4s - loss: 0.082 - ETA: 3s - loss: 0.074 - ETA: 3s - loss: 0.074 - ETA: 3s - loss: 0.066 - ETA: 3s - loss: 0.070 - ETA: 3s - loss: 0.065 - ETA: 3s - loss: 0.062 - ETA: 3s - loss: 0.063 - ETA: 2s - loss: 0.063 - ETA: 2s - loss: 0.060 - ETA: 2s - loss: 0.058 - ETA: 2s - loss: 0.062 - ETA: 2s - loss: 0.067 - ETA: 2s - loss: 0.068 - ETA: 2s - loss: 0.064 - ETA: 1s - loss: 0.062 - ETA: 1s - loss: 0.063 - ETA: 1s - loss: 0.060 - ETA: 1s - loss: 0.062 - ETA: 1s - loss: 0.062 - ETA: 1s - loss: 0.064 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.062 - 4s 123ms/step - loss: 0.0620\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.619625357321295 - mean_average_precision(0.0): 0.6191039894626392\n",
      "Epoch 19/20\n",
      "35/35 [==============================] - ETA: 1:43 - loss: 0.040 - ETA: 53s - loss: 0.037 - ETA: 36s - loss: 0.05 - ETA: 27s - loss: 0.04 - ETA: 22s - loss: 0.03 - ETA: 18s - loss: 0.03 - ETA: 16s - loss: 0.03 - ETA: 14s - loss: 0.03 - ETA: 12s - loss: 0.04 - ETA: 11s - loss: 0.04 - ETA: 10s - loss: 0.04 - ETA: 9s - loss: 0.0463 - ETA: 8s - loss: 0.046 - ETA: 7s - loss: 0.047 - ETA: 7s - loss: 0.045 - ETA: 6s - loss: 0.042 - ETA: 6s - loss: 0.051 - ETA: 5s - loss: 0.049 - ETA: 5s - loss: 0.053 - ETA: 4s - loss: 0.053 - ETA: 4s - loss: 0.053 - ETA: 3s - loss: 0.051 - ETA: 3s - loss: 0.054 - ETA: 3s - loss: 0.052 - ETA: 2s - loss: 0.050 - ETA: 2s - loss: 0.054 - ETA: 2s - loss: 0.053 - ETA: 1s - loss: 0.056 - ETA: 1s - loss: 0.055 - ETA: 1s - loss: 0.056 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.054 - ETA: 0s - loss: 0.052 - ETA: 0s - loss: 0.052 - 8s 217ms/step - loss: 0.0528\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6191123363877359 - normalized_discounted_cumulative_gain@5(0.0): 0.6195796020238113 - mean_average_precision(0.0): 0.6189658540396937\n",
      "Epoch 20/20\n",
      "35/35 [==============================] - ETA: 1:07 - loss: 0.015 - ETA: 1:08 - loss: 0.042 - ETA: 1:08 - loss: 0.064 - ETA: 1:06 - loss: 0.048 - ETA: 1:04 - loss: 0.041 - ETA: 58s - loss: 0.037 - ETA: 48s - loss: 0.05 - ETA: 41s - loss: 0.04 - ETA: 36s - loss: 0.09 - ETA: 31s - loss: 0.08 - ETA: 28s - loss: 0.08 - ETA: 24s - loss: 0.07 - ETA: 22s - loss: 0.08 - ETA: 19s - loss: 0.07 - ETA: 17s - loss: 0.07 - ETA: 16s - loss: 0.06 - ETA: 14s - loss: 0.06 - ETA: 13s - loss: 0.06 - ETA: 11s - loss: 0.06 - ETA: 10s - loss: 0.06 - ETA: 9s - loss: 0.0605 - ETA: 8s - loss: 0.058 - ETA: 7s - loss: 0.059 - ETA: 6s - loss: 0.057 - ETA: 5s - loss: 0.060 - ETA: 5s - loss: 0.058 - ETA: 4s - loss: 0.056 - ETA: 3s - loss: 0.056 - ETA: 3s - loss: 0.056 - ETA: 2s - loss: 0.055 - ETA: 1s - loss: 0.055 - ETA: 1s - loss: 0.058 - ETA: 0s - loss: 0.059 - ETA: 0s - loss: 0.059 - 16s 444ms/step - loss: 0.0606\n",
      "Validation: normalized_discounted_cumulative_gain@3(0.0): 0.6194423495565282 - normalized_discounted_cumulative_gain@5(0.0): 0.619625357321295 - mean_average_precision(0.0): 0.6190830598531021\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, epochs=20, callbacks=[evaluate], workers=2, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mean_average_precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-93f933a2ca4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Plot training & validation accuracy values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_average_precision'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mean_average_precision'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAe5UlEQVR4nO3deXRcZ53m8e9PS0kq7ZZkWasXrMR2HG8RiSELoc2SpBMbGKDjEAgQ4qZ7MoRpZs6EwwzDwJw5LGfomeGEhjgJMAGcsGNoZ0Kg09nATuR4SWLHjuNVlizZlrUvVSW980eV5IosWWW7pFvL8zmnTt269y3Vz9dXj2699773mnMOERFJfhleFyAiIvGhQBcRSREKdBGRFKFAFxFJEQp0EZEUkeXVB5eXl7t58+Z59fEiIklp+/btp5xzFRMt8yzQ582bR1NTk1cfLyKSlMzsyGTL1OUiIpIiFOgiIilCgS4ikiIU6CIiKUKBLiKSIhToIiIpQoEuIpIiki7Qmw538PUnXkeX/RUReaukC/TXWrr53jNv0tI16HUpIiIJJekCfVV9KQAvHznjcSUiIokl6QJ9UVUhudkZ7Dja6XUpIiIJJekCPTszg2U1Jbx8VHvoIiLRki7QAVbWl7CnpZuh0LDXpYiIJIwkDfRSAsMjvHq82+tSREQSRlIG+qr6EgB2qNtFRGRMUgb67KJcakrydGBURCRKUgY6wKq5pTowKiISJWkDfWVdCa1dg7R2DXhdiohIQkjaQF81NzzASN0uIiJhSRvoS6qK8GVlaMSoiEhE0ga6LyuDK2uK2XFMe+giIpDEgQ7h0xdfOd5FIDTidSkiIp5L6kBfWV9KIDTCnlYNMBIRSepA15UXRUTOSupAn1OcS1Vxrs5HFxEhyQMdwnvpOnVRRCQFAn1lfQnHOwdo79YdjEQkvaVAoEf60bWXLiJpLukDfWlNEb7MDF15UUTSXtIHek5WJkuqi9SPLiJpL+kDHcIHRncf7yQ4rAFGIpK+UiPQ55YwGBxhrwYYiUgaS4lAHz0wqm4XEUlnMQW6md1kZvvM7ICZ3T/B8noze9rMdpjZbjO7Jf6lTq66OJfKohwNMBKRtDZloJtZJvAAcDOwBFhvZkvGNfvPwM+ccyuB24HvxrvQKWpkZZ0GGIlIeotlD/1q4IBz7qBzLgA8Bqwb18YBRZHpYqAlfiXGZtXcEo529HOqd2imP1pEJCHEEug1wLGo182RedG+AtxpZs3AFuDfTfSDzGyDmTWZWdPJkycvotzJ6UJdIpLuYgl0m2CeG/d6PfBD51wtcAvwqJmd87Odcw865xqdc40VFRUXXu15LK0pJivDdMMLEUlbsQR6M1AX9bqWc7tU7gZ+BuCc+wuQC5THo8BY5WZnckV1kfbQRSRtxRLoLwENZjbfzHyED3puHtfmKLAGwMwWEw70+PapxGBlfSm7m7sIaYCRiKShKQPdORcC7gWeBPYSPpvlNTP7qpmtjTT7AnCPme0CNgGfdM6N75aZdivrSxgIDvP6iZ6Z/mgREc9lxdLIObeF8MHO6HlfjpreA1wb39Iu3KqxAUZnWFpT7HE1IiIzKyVGio6qLc2jvCBH56OLSFpKqUA3M1bVl2jEqIikpZQKdAgfGD18up+OvoDXpYiIzKiUC/RV9SUAuuGFiKSdlAv0K2uLycwwdbuISNpJuUD3+7JYXFWoA6MiknZSLtAhfPrirmOdDI/M+KnwIiKeSclAX1lfQl9gmP1tGmAkIukjJQN97MqL6kcXkTSSkoFeP8vPrHwfLx9RP7qIpI+UDPTRAUY7jmkPXUTSR0oGOoQHGB082UdnvwYYiUh6SOFAjwww0g0vRCRNpGygL68tIcNgh254ISJpImUDPT8ni8vnFPGyBhiJSJpI2UCH8HVddmqAkYikiZQO9JX1pfQOhTjQ3ut1KSIi0y6lA11XXhSRdJLSgT6/PJ8Sf7ZGjIpIWkjpQDczVtaV6MCoiKSFlA50CF/X5UB7L10DQa9LERGZVikf6CsjF+raqQFGIpLiUj7Ql9cVY6YDoyKS+lI+0Atzs7m8slD96CKS8lI+0CF8XZedR88wogFGIpLC0iTQS+keDHHwlAYYiUjqSotAHx1gpBteiEgqS4tAX1BeQFFulm54ISIpLS0CPSPDWFlfqj10EUlpaRHoED4wur+9h55BDTASkdSUNoG+qr4U52DXsS6vSxERmRZpE+jL6yIHRjXASERSVNoEenFeNg2zCzRiVERSVtoEOoT70Xcc68Q5DTASkdSTVoG+qr6Uzv4gh071eV2KiEjcpVegzw1feVHXdRGRVBRToJvZTWa2z8wOmNn9k7T5qJntMbPXzOyn8S0zPhZWFFCYk6UDoyKSkrKmamBmmcADwHuBZuAlM9vsnNsT1aYB+CJwrXPujJnNnq6CL0VGhrGivoQd2kMXkRQUyx761cAB59xB51wAeAxYN67NPcADzrkzAM659viWGT8r60vZd6Kbrn4NMBKR1BJLoNcAx6JeN0fmRbsMuMzMXjCzrWZ2U7wKjLc1i2Yz4uDJ1054XYqISFzFEug2wbzx5/1lAQ3AjcB64CEzKznnB5ltMLMmM2s6efLkhdYaF8tqi5lb5mfzrhZPPl9EZLrEEujNQF3U61pgfBo2A791zgWdc4eAfYQD/i2ccw865xqdc40VFRUXW/MlMTPWLq/mz2+eor1n0JMaRESmQyyB/hLQYGbzzcwH3A5sHtfmN8C7AcysnHAXzMF4FhpPa5dXM+Jgy+5Wr0sREYmbKQPdORcC7gWeBPYCP3POvWZmXzWztZFmTwKnzWwP8DTwH51zp6er6EvVUFnIojmF6nYRkZQy5WmLAM65LcCWcfO+HDXtgH+IPJLC2hXVfPP/7eNYRz91s/xelyMicsnSaqRotNuWVQPwu93aSxeR1JC2gV43y8+q+hI271Sgi0hqSNtAh/DB0ddP9LC/rcfrUkRELllaB/pfL6smw9BeuoikhLQO9IrCHK5dWM7mXS26RrqIJL20DnSA25ZXc7Sjn13NuteoiCS3tA/0918xB19mhrpdRCTppX2gF+dlc+PlFfx+dwvDI+p2EZHklfaBDuFBRu09Q2w7lLCDW0VEpqRAB9YsqiTfl8nvdCkAEUliCnQgz5fJe5dUsuWVEwRCI16XIyJyURToEWtXVNM1EOS5N7y5TruIyKVSoEdct7CCEn+2rsAoIklLgR7hy8rg5qVVPLWnjYHAsNfliIhcMAV6lLXLq+kPDPPHvW1elyIicsEU6FGunj+LyqIcdbuISFJSoEfJzDBuXVbNM/tO0jUQ9LocEZELokAfZ+3yagLDIzz56gmvSxERuSAK9HGW1RYzt8yvbhcRSToK9HHMjLXLq/nzm6do7xn0uhwRkZgp0Cewdnk1Iw627G71uhQRkZgp0CfQUFnIojmF/FbdLiKSRBTok1i7opodRzs51tHvdSkiIjFRoE/itmXVADo4KiJJQ4E+ibpZflbVl+iSuiKSNBTo57FuRQ2vn+hhf1uP16WIiExJgX4et1xZRYah+42KSFJQoJ9HRWEO1y4sZ/OuFpzT/UZFJLEp0Kdw2/Jqjnb0s6u5y+tSRETOS4E+hfdfMQdfZoa6XUQk4SnQp1Ccl82Nl1fw+90tDI+o20VEEpcCPQZrV1TT3jPEtkOnvS5FRGRSCvQYrFlUSb4vU+eki0hCU6DHIM+XyXuXVLLllRMEQiNelyMiMiEFeozWrqimayDIc2+c9LoUEZEJKdBjdN3CCkr82bq2i4gkLAV6jHxZGdy8tIqn9rQxEBj2uhwRkXPEFOhmdpOZ7TOzA2Z2/3nafdjMnJk1xq/ExLF2eTX9gWH+uLfN61JERM4xZaCbWSbwAHAzsARYb2ZLJmhXCHwO2BbvIhPF1fNnUVmUo24XEUlIseyhXw0ccM4ddM4FgMeAdRO0+xrwTSBlb8SZmWHcuqyaf93Xzu7mTq/LERF5i1gCvQY4FvW6OTJvjJmtBOqcc78/3w8ysw1m1mRmTSdPJufZIvdcv4DKolzufGgbr+j6LiKSQGIJdJtg3tgYeDPLAP4R+MJUP8g596BzrtE511hRURF7lQlkTnEuj21YTVFeNnc+vI1XjyvURSQxxBLozUBd1OtaILoTuRBYCvyrmR0GVgObU/XAKEBtqZ9N96ymICeLjz2kUBeRxBBLoL8ENJjZfDPzAbcDm0cXOue6nHPlzrl5zrl5wFZgrXOuaVoqThB1s/w8tiEc6nc+vI3XWhTqIuKtKQPdORcC7gWeBPYCP3POvWZmXzWztdNdYCKrmxXeU/dnZ/KxhxTqIuIt8+pOPI2Nja6pKTV24o+e7uf2B/9Cf3CYn35mNUuqi7wuSURSlJltd85N2KWtkaJxUF/mZ9OG1eRlZ/Kxh7ayt7Xb65JEJA0p0ONkblk+m+5ZTU5WuPvl9RMKdRGZWQr0OJpXns9jG1bjy8zgjo3b2Heix+uSRCSNKNDjbF55Pps2rCY707hj41aFuojMGAX6NJhfHu5+ycwIh/r+NoW6iEw/Bfo0WVBRwKYNZ0P9DYW6iEwzBfo0elsk1M2M9Ru3caBdoS4i00eBPs3eVlHApntWYwa3P7iNA+29XpckIilKgT4DFs4uYNM91wCwfuNWhbqITAsF+gxZOLuQTfdcg3OO9Ru3cvCkQl1E4kuBPoMaKgvZdM9qRkYcd2zcxpHTfV6XJCIpRIE+wxoqC/nxZ65hMDTMHRu30Xym3+uSRCRFKNA9sLiqiB/ffQ09g0HWb9xKa9eA1yWJSApQoHtkaU0xj959DZ19Qe7YuI327pS9FauIzBAFuoeW15Xww0+/nfbuQdZv3MrJniGvSxKRJKZA99hVc2fxyCffTkvnIHc+tI2OvoDXJYlIklKgJ4BrFpTx0F2NHD7dx50PbaOzX6EuIhdOgZ4grl1YzoOfaORAey+feORFugeDXpckIklGgZ5A3nVZBf905yr2tnZz1yMv0jsU8rokEUkiCvQEs2ZxJd9Zv4rdzV186gcv0h9QqItIbBToCeimpXP437evYPuRM9z9wyYGAsNelyQiSUCBnqBuXVbNtz+6gq2HTrPh0SYGgwp1ETk/BXoC+8DKGr7xb5bx3Bun+Lsfb2copFAXkckp0BPcRxvr+B8fvJKn953k3p/uIDg84nVJIpKgFOhJ4I5r6vlva6/gqT1t3PfYDkIKdRGZQJbXBUhs7nrnPILDI/z3f95Lhu3ka+uWUprv87osEUkgCvQk8pnrFxAacXz9idf50952PtpYy93XLaC+zO91aSKSAMw558kHNzY2uqamJk8+O9ntb+th47MH+c3O4wyPOG5eWsWGGxawvK7E69JEZJqZ2XbnXOOEyxToyaute5AfvHCYn2w7Qs9giKvnz+Jvb1jAuy+fTUaGeV2eiEwDBXqK6x0K8diLR3nk+UO0dA2ycHYB91w/nw+srCEnK9Pr8kQkjhToaSI4PMKWV1r5/jMH2dPaTUVhDp985zzuvGYuxf5sr8sTkThQoKcZ5xwvHDjN9599k+feOIXfl8nfvL2Ou6+bT22pDqCKJDMFehrb29rNxmcPsnlXCw645coq/vaGBSytKfa6NBG5CAp0oaVzgB+8cIhNLx6jdyjE3zTW8cVbFlHi17nsIslEgS5jugeDPPAvB3jo+UOU5GXzX25dwroV1ZjprBiRZHC+QNfQ/zRTlJvNF29ZzO/uvY66WX4+//hOPv7wixw+1ed1aSJyiWIKdDO7ycz2mdkBM7t/guX/YGZ7zGy3mf3JzObGv1SJpyXVRfzy797J19Zdwa5jnbz/fz3LA08fIBDSdWJEktWUgW5mmcADwM3AEmC9mS0Z12wH0OicWwb8AvhmvAuV+MvMMD7+jnn88QvvYs3i2XzryX3c+p3naDrc4XVpInIRYtlDvxo44Jw76JwLAI8B66IbOOeeds71R15uBWrjW6ZMp8qiXL77sat4+K5G+oaG+fD3/sIXf/UKXf26UbVIMokl0GuAY1GvmyPzJnM38MREC8xsg5k1mVnTyZMnY69SZsSaxZX84d/fwD3Xz+fxl46y5tvPhE939OjAuYhcmFgCfaLTHyb8DTezO4FG4FsTLXfOPeica3TONVZUVMRepcyY/JwsvvTXS9h873VUl+TyuU07+OQPXuJYR//UbxYRT8US6M1AXdTrWqBlfCMzew/wJWCtc24oPuWJV5bWFPPrv7+W/3rbEpoOd/Def3yG7z3zpu6YJJLAYgn0l4AGM5tvZj7gdmBzdAMzWwl8n3CYt8e/TPFCZobxqWvn88cvvIsbGir4+hOvc9t3nuflo2e8Lk1EJjBloDvnQsC9wJPAXuBnzrnXzOyrZrY20uxbQAHwczPbaWabJ/lxkoSqivN48BONfP/jV9E1EORD3/0zn310O3taur0uTUSiaKSoXJDeoRAPPnuQHzx/iJ6hEO9bUsnn1jTo2jAiM0RD/yXuuvqDPPLCIR554RA9gyHeu6SS+xTsItNOgS7TpmsgyA9fOMzDzx+kezDEexbP5r41l3FlrYJdZDoo0GXadQ8G+dELh3no+UN0DQT5q0WzuW9Ng+5zKhJnCnSZMT2DQX7053Cwd/YHefflFdz3nstYoWAXiQsFusy43qFQONifO8iZ/iDvuqyC+97TwKr6Uq9LE0lqCnTxTO9QiEf/coSNzx2koy/A9Q3l3Lemgavmluoa7CIXQYEunusbCvHo1iNsfPYgp/sCLJpTyIevquWDK2soK8jxujyRpKFAl4TRHwjxy5eP84umY+xq7iIrw/irRbP5SGMdN15eQXam7rkicj4KdElI+9t6+MX2Zn718nFO9Q5RXuDjAytq+EhjHZfPKfS6PJGEpECXhBYcHuGZfSf5+fZj/GlvO6ERx7LaYj5yVS1rl9dQ7M/2ukSRhKFAl6RxuneI3+5s4efbm9nb2o0vK4P3Lankw1fVcn1DBZkZOpAq6U2BLknp1eNd/GJ7M7/ZeZzO/iBzinL50Koa1q2oobIohzxfJr7MDJ0tI2lFgS5JbSg0zJ/2tvPzpmM8s/8kI1GbbGaG4c/OJM+Xid+XSZ4vi3zf2dd+X1Z4Ovvs8sLcLBaU59NQWUh5gU9/ECSpnC/Qs2a6GJELlZOVyS1XVnHLlVW0dw/yzP6T9AyGGAgO0x8I0R8YZiAwTH/g7OuewRDt3UP0B0NjywaCw4zffyn1Z9NQWchllQVcVllIw+zwtE6llGSkQJekMrsol4801k3dcALOOQaDI3QOBHizvY/9bT280d7D/rZefruzhZ7B0FjbsnwfDaMhX1nIZbPD06X5vnj9U0TiToEuacPMyPNlkufLo6o4j+sayseWOedo6x5if1tPOOjbetnf3sOvXj5O79DZoC8vyKFhdgHzK/KZX5bP3DI/88vzqZvlJzc704t/lsgYBboI4bCfU5zLnOJcbrjs7A3MnXO0dg2eDfm2Hva397LllVY6+4NR74fq4jzmlfuZV5YffpTnM7/cT22pwl5mhgJd5DzMjOqSPKpL8rjx8tlvWdbZH+Dw6X4On+rj0Kk+jpzu49Dpfv55irCfX57PojlFLK4qVF+9xJUCXeQilfh9rPD7Jrw0cGd/IBLy/W8J+9/vbqVr4GzYzy7MYXFVEUuqi8LPVYXMLy/Q+fZyURToItOgxO9jZb2PlRNcLvh07xCvn+hhb2s3e1q72dvaw5+fO0hwOHwKTk5WBovmFLK4qmjssaiqkKJcjZiV89N56CIJIBAa4UB7L3tbu8OPE93saenmTFTXTd2sPBbPKWJRVRHVxblUFOYwuzD8XFbgm7ELmw2POH2D8JDOQxdJcL6sDJZUh7teRo2eeXN2Tz78/NTetnPOpzeDWX4fFYU5Y4/RsA9Pn30uyMkiNOLoGgjS2R+kayBI90D4efQxOn/8ss6BAIPBEcoLfNTP8p99lOWPTc8uzCFDge8J7aGLJJlAaIRTvUO09wxxsmeI9p7ByPPQ2POpyHRgeOSc92dn2lj3zmTyfZkU52VT7PdRnJcVno48/L4s2roHOdrRz5HT/bR2Dbxl9G5OVgZ10WEfecwt8+v0zjjQHrpICvFlZYydeXM+zoX3wscHf0dfMBzY/nBAF0WCuiTv7OsL6b4JhEZo6RzgSEc/Rzv6OdbRz5HTfRztGGDbwdP0BYbf0r6iMIfCnCx8WRnkZmeSmx1+zhl9nRWel5OdSW5W5DlqeV52JqX+bMoLcyjL91Hq9+kbQYQCXSRFmRklfh8lfh+XVU7f9eV9WRnMKw+fdz+ec46OvgBHI2F/9HQ/zWcG6AuEGAyOMBQaZig4QkdfgMHg8Ni8weAIg8FhhkLnfsMYLzPDKPX7KC/wUV6QQ3mBj7KCHMoLwscWKiLPo69zslL3G4ICXUSmjZlRVpBDWUHOhGf8TMU5x1BohKHgCIOhYQaD4WvydPQFON0b4FTv0NjzqcjzkY4+TvUEGAgOT/gzC3OzqCnJG+sWqis9O11b6ifPF7/A7x4M0to5SGvXAK1dg7R2hp8/tKqWd7ytLG6fM0qBLiIJy8wi3TKZFHNhp232B0Kc6glwqm8oKvzD3U/NZwY4crqP5944yWDwrd8CygtyqJ8VHfh+amflUT/LT1Vx3tgZPr1DobGAbu0aoKVzkBNdg7REwvtE1+BbLhsR/veExx68c2H8wxwU6CKSovy+LOrLsqgv80/axjnHqd4Ax86E+/6PjR0HGGD7kTP8blfLWw74ZmUYlUW5dA8G33IxNwiHdXlBDtXFuSysKOC6heVUl+RSVZxHVXEuVSV5zC7MmdbTSxXoIpK2zGzs1M5VE3QJBYdHaO0c5NiZswd8WzoHKM7LpqokEtSRwK4sysWX5e1NzhXoIiKTyM7MoL7MT32Zn2u9LiYG3v45ERGRuFGgi4ikCAW6iEiKUKCLiKQIBbqISIpQoIuIpAgFuohIilCgi4ikCM+uh25mJ4EjF/n2cuBUHMuJN9V3aVTfpUv0GlXfxZvrnKuYaIFngX4pzKxpsgu8JwLVd2lU36VL9BpV3/RQl4uISIpQoIuIpIhkDfQHvS5gCqrv0qi+S5foNaq+aZCUfegiInKuZN1DFxGRcRToIiIpIqED3cxuMrN9ZnbAzO6fYHmOmT0eWb7NzObNYG11Zva0me01s9fM7L4J2txoZl1mtjPy+PJM1Rf5/MNm9krks5smWG5m9n8i62+3ma2awdouj1ovO82s28w+P67NjK8/M3vEzNrN7NWoebPM7CkzeyPyPOHdjs3srkibN8zsrhmq7Vtm9nrk/+/XZlYyyXvPuy1Mc41fMbPjUf+Pt0zy3vP+vk9jfY9H1XbYzHZO8t4ZWYeXxDmXkA8gE3gTWAD4gF3AknFt/h74XmT6duDxGayvClgVmS4E9k9Q343A7z1ch4eB8vMsvwV4AjBgNbDNw//rE4QHTHi6/oAbgFXAq1HzvgncH5m+H/jGBO+bBRyMPJdGpktnoLb3AVmR6W9MVFss28I01/gV4D/EsA2c9/d9uuobt/x/Al/2ch1eyiOR99CvBg445w465wLAY8C6cW3WAT+KTP8CWGNmNhPFOedanXMvR6Z7gL1AzUx8dhytA/6vC9sKlJhZlQd1rAHedM5d7MjhuHHOPQt0jJsdvZ39CPjABG99P/CUc67DOXcGeAq4abprc879wTk3erfirUBtPD/zQk2y/mIRy+/7JTtffZHs+CiwKd6fO1MSOdBrgGNRr5s5NzDH2kQ26i6gbEaqixLp6lkJbJtg8TvMbJeZPWFmV8xoYeCAP5jZdjPbMMHyWNbxTLidyX+JvFx/oyqdc60Q/kMOzJ6gTSKsy08T/sY1kam2hel2b6Rb6JFJuqwSYf1dD7Q5596YZLnX63BKiRzoE+1pjz/HMpY208rMCoBfAp93znWPW/wy4W6E5cB3gN/MZG3Atc65VcDNwL81sxvGLU+E9ecD1gI/n2Cx1+vvQni6Ls3sS0AI+MkkTabaFqbTPwFvA1YArYS7NcbzfFsE1nP+vXMv12FMEjnQm4G6qNe1QMtkbcwsCyjm4r7uXRQzyyYc5j9xzv1q/HLnXLdzrjcyvQXINrPymarPOdcSeW4Hfk34a220WNbxdLsZeNk51zZ+gdfrL0rbaFdU5Ll9gjaercvIAdhbgY+5SGfveDFsC9PGOdfmnBt2zo0AGyf5bE+3xUh+fAh4fLI2Xq7DWCVyoL8ENJjZ/Mhe3O3A5nFtNgOjZxN8GPiXyTboeIv0tz0M7HXOfXuSNnNG+/TN7GrC6/v0DNWXb2aFo9OED569Oq7ZZuATkbNdVgNdo10LM2jSvSIv19840dvZXcBvJ2jzJPA+MyuNdCm8LzJvWpnZTcB/AtY65/onaRPLtjCdNUYfl/ngJJ8dy+/7dHoP8LpzrnmihV6vw5h5fVT2fA/CZ2HsJ3z0+0uReV8lvPEC5BL+qn4AeBFYMIO1XUf4K+FuYGfkcQvwWeCzkTb3Aq8RPmK/FXjnDNa3IPK5uyI1jK6/6PoMeCCyfl8BGmf4/9dPOKCLo+Z5uv4I/3FpBYKE9xrvJnxc5k/AG5HnWZG2jcBDUe/9dGRbPAB8aoZqO0C473l0Gxw966sa2HK+bWEG19+jke1rN+GQrhpfY+T1Ob/vM1FfZP4PR7e7qLaerMNLeWjov4hIikjkLhcREbkACnQRkRShQBcRSREKdBGRFKFAFxFJEQp0EZEUoUAXEUkR/x+KvBN+2Z2VxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['mean_average_precision'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
