{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk.util import ngrams\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:29:13.013116\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features for dataset\n",
    "#df = pd.concat([pd.read_csv(f, delimiter='\\t') for f in glob.glob('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/data/user-ct-test-collection-*.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled dataset\n",
    "This sample dataset is put on 1.000.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples = df.sample(1000000, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save samples in pickle file & CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples.to_pickle('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/sample_million.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_pickle('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/sample_million.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create N-gram & NER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_candidate(candidate):\n",
    "    line = re.sub(r\"[,.;@#?!&$]+\\ *\", \" \", str(candidate))\n",
    "    return line\n",
    "\n",
    "samples['Query_clean'] = samples[samples.notnull()]['Query'].map(lambda query: clean_candidate(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = []\n",
    "\n",
    "for row in samples.itertuples():\n",
    "    words = str(row.Query_clean).split()\n",
    "    for j in range(0, len(words)):\n",
    "        suffix = \" \".join(words[j:])\n",
    "        suffixes.append(suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = gzip.GzipFile(\"/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/suffixes.npy.gz\", \"w\")\n",
    "#np.save(file=f, arr=suffixes)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = gzip.GzipFile('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/pickles/suffixes.npy.gz', \"r\")\n",
    "# suffixes = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create N-gram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_dict = collections.Counter(suffixes)\n",
    "\n",
    "def ngram_freq_per_n(candidate, historical_dict, n):\n",
    "    words = candidate.split()\n",
    "    ngram_n = 0\n",
    "    ngrams_i = ngrams(words, n)\n",
    "    \n",
    "    for word in ngrams_i:\n",
    "        freq_g = historical_dict[\" \".join(word)]\n",
    "        ngram_n += freq_g\n",
    "        \n",
    "    return ngram_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = gzip.GzipFile(\"/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/historical_dict.npy.gz\", \"w\")\n",
    "#np.save(file=f, arr=historical_dict)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = gzip.GzipFile('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/pickles/historical_dict.npy.gz', \"r\")\n",
    "# historical_dict = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_max = 6\n",
    "\n",
    "for i in range(1, ngram_max + 1):\n",
    "    ngram_name = 'ngram_' + str(i)\n",
    "    samples[ngram_name] = samples[samples.notnull()]['Query_clean'].map(lambda candidate_row: ngram_freq_per_n(candidate_row, historical_dict, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create NER Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_contains_and_count_norm(candidate): \n",
    "    entities = nlp(str(candidate))\n",
    "    contains = 0\n",
    "    ner_norm = 0\n",
    "    \n",
    "    if len(entities) > 0:\n",
    "        contains = 1\n",
    "        ner_norm = len(entities) / len(candidate)\n",
    "    \n",
    "    return [contains, ner_norm]\n",
    "\n",
    "samples['has_ne'], samples['ne_norm'] = zip(*samples[samples.notnull()]['Query_clean'].map(lambda candidate_row: ner_contains_and_count_norm(candidate_row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_artificial_queries(samples): \n",
    "    artif_list = []\n",
    "\n",
    "    for row in samples.itertuples():\n",
    "        words = str(row.Query_clean).split()\n",
    "        # iterate #-filtered times\n",
    "        for j in range(1, len(words)):\n",
    "            # Last two will be filled as [6]: 'prefix', [7]'suffix'\n",
    "            temp_list = [row.Index, row.AnonID, row.QueryTime, row.ItemRank, row.ClickURL, row.Query, row.Query_clean, '', '']\n",
    "            prefix = \" \".join(words[:j])\n",
    "            temp_list[6] = prefix\n",
    "            suffix = \" \".join(words[j:])\n",
    "            temp_list[7] = suffix\n",
    "            # Add to artificial query list\n",
    "            artif_list.append(temp_list)   \n",
    "    return artif_list\n",
    "\n",
    "artif_list = create_artificial_queries(samples)\n",
    "queries = pd.DataFrame.from_records(artif_list)\n",
    "queries.columns = ['Index', 'AnonID', 'QueryTime', 'ItemRank', 'ClickURL', 'Query', 'Query_clean', 'Prefix', 'Suffix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create all prefix and suffix combinations for queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all queries shorter than 2\n",
    "def split_words(string):\n",
    "    words = str(string).split()\n",
    "    return len(words)\n",
    "\n",
    "samples['filtered'] = samples[samples.notnull()]['Query_clean'].map(lambda query: split_words(query))\n",
    "samples = samples[samples.filtered > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_features(prefix, suffix, historical_dict):     \n",
    "    # The complete query\n",
    "    complete = \"\"\n",
    "#     if bool_space:\n",
    "#         complete = prefix + suffix\n",
    "#     else:\n",
    "#         complete = prefix + \" \" + suffix\n",
    "    \n",
    "    # The frequency of the candidate in the historical logs\n",
    "    frequency = historical_dict[complete]\n",
    "    \n",
    "    # Prefix, suffix and total length in characters\n",
    "    prefixlen_char = len(prefix)\n",
    "    suffixlen_char = len(suffix)\n",
    "    totallen_char = len(complete)\n",
    "    \n",
    "    # Prefix, suffix and total length in words\n",
    "    prefixlen_word = len(prefix.split())\n",
    "    suffixlen_word = len(suffix.split())\n",
    "    totallen_word = len(complete.split())\n",
    "    \n",
    "    return [frequency, \n",
    "            prefixlen_char, suffixlen_char, totallen_char,\n",
    "            prefixlen_word, suffixlen_word, totallen_word]\n",
    "\n",
    "queries['candid_freq'], queries['prefixlen_char'], queries['suffixlen_char'], queries['totallen_char'], queries['prefixlen_word'], queries['suffixlen_word'], queries['totallen_word'] = zip(*queries.apply(lambda query_row: get_other_features(query_row.Prefix, query_row.Suffix, historical_dict), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of            Index    AnonID            QueryTime ItemRank  \\\n",
       "0        2291693   7450444  2006-03-26 19:50:45      NaN   \n",
       "1         852212   2220588  2006-05-30 06:11:21      NaN   \n",
       "2         852212   2220588  2006-05-30 06:11:21      NaN   \n",
       "3         204378    516545  2006-05-07 09:10:15      NaN   \n",
       "4         204378    516545  2006-05-07 09:10:15      NaN   \n",
       "...          ...       ...                  ...      ...   \n",
       "1782832  2775561  10719443  2006-05-10 23:22:52        1   \n",
       "1782833  2775561  10719443  2006-05-10 23:22:52        1   \n",
       "1782834    79589    196642  2006-04-04 12:09:49       11   \n",
       "1782835   169411    401787  2006-05-31 11:11:36        1   \n",
       "1782836   169411    401787  2006-05-31 11:11:36        1   \n",
       "\n",
       "                       ClickURL                       Query   Query_clean  \\\n",
       "0                           NaN                  chat rooms          chat   \n",
       "1                           NaN      aol keyword aol-dsl104           aol   \n",
       "2                           NaN      aol keyword aol-dsl104   aol keyword   \n",
       "3                           NaN  moen faucets official site          moen   \n",
       "4                           NaN  moen faucets official site  moen faucets   \n",
       "...                         ...                         ...           ...   \n",
       "1782832   http://www.saturn.com              www.saturn.com           www   \n",
       "1782833   http://www.saturn.com              www.saturn.com    www saturn   \n",
       "1782834    http://www.joggs.com                   big boobs           big   \n",
       "1782835  http://www.xandria.com             www.xandria.com           www   \n",
       "1782836  http://www.xandria.com             www.xandria.com   www xandria   \n",
       "\n",
       "                        Prefix Suffix  candid_freq  prefixlen_char  \\\n",
       "0                        rooms                   0               5   \n",
       "1           keyword aol-dsl104                   0              18   \n",
       "2                   aol-dsl104                   0              10   \n",
       "3        faucets official site                   0              21   \n",
       "4                official site                   0              13   \n",
       "...                        ...    ...          ...             ...   \n",
       "1782832             saturn com                   0              10   \n",
       "1782833                    com                   0               3   \n",
       "1782834                  boobs                   0               5   \n",
       "1782835            xandria com                   0              11   \n",
       "1782836                    com                   0               3   \n",
       "\n",
       "         suffixlen_char  totallen_char  prefixlen_word  suffixlen_word  \\\n",
       "0                     0              0               1               0   \n",
       "1                     0              0               2               0   \n",
       "2                     0              0               1               0   \n",
       "3                     0              0               3               0   \n",
       "4                     0              0               2               0   \n",
       "...                 ...            ...             ...             ...   \n",
       "1782832               0              0               2               0   \n",
       "1782833               0              0               1               0   \n",
       "1782834               0              0               1               0   \n",
       "1782835               0              0               2               0   \n",
       "1782836               0              0               1               0   \n",
       "\n",
       "         totallen_word  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "...                ...  \n",
       "1782832              0  \n",
       "1782833              0  \n",
       "1782834              0  \n",
       "1782835              0  \n",
       "1782836              0  \n",
       "\n",
       "[1782837 rows x 16 columns]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.to_pickle(r'/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/created_sample/artif_queries_dataset_big.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:06:50.605594\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
