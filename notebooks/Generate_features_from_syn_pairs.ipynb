{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import datetime\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from nltk.util import ngrams\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "path_to_folder = '/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:08:39.366110\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create featureset\n",
    "Load the dataset with the generated synthetic candidate queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_candidate_queries = pd.read_pickle(path_to_folder + 'created_sample/syn_candidate_queries_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create N-gram & NER features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_folder + 'created_sample/hist_suffixes.pickle_1m', 'rb') as f:\n",
    "    suffixes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create N-gram features based on the \"Query_clean\"-column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_dict = collections.Counter(suffixes)\n",
    "\n",
    "def ngram_freq_per_n(candidate, historical_dict, n):\n",
    "    words = candidate.split()\n",
    "    ngram_n = 0\n",
    "    ngrams_i = ngrams(words, n)\n",
    "    \n",
    "    for word in ngrams_i:\n",
    "        freq_g = historical_dict[\" \".join(word)]\n",
    "        ngram_n += freq_g\n",
    "        \n",
    "    return ngram_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 1093771/1093771 [00:05<00:00, 202620.12it/s]\n",
      "100%|██████████| 1093771/1093771 [00:05<00:00, 193652.02it/s]\n",
      "100%|██████████| 1093771/1093771 [00:05<00:00, 211090.97it/s]\n",
      "100%|██████████| 1093771/1093771 [00:04<00:00, 227344.13it/s]\n",
      "100%|██████████| 1093771/1093771 [00:04<00:00, 239688.30it/s]\n",
      "100%|██████████| 1093771/1093771 [00:04<00:00, 248155.01it/s]\n"
     ]
    }
   ],
   "source": [
    "ngram_max = 6\n",
    "\n",
    "tqdm.pandas()  # <- added this for timer\n",
    "\n",
    "for i in range(1, ngram_max + 1):\n",
    "    ngram_name = 'ngram_' + str(i)\n",
    "    syn_candidate_queries[ngram_name] = syn_candidate_queries[syn_candidate_queries.notnull()]['Query_clean'].progress_map(lambda candidate_row: ngram_freq_per_n(candidate_row, historical_dict, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create NER Features based on the \"Synthetic_query\"-column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1093771/1093771 [1:48:24<00:00, 168.15it/s] \n"
     ]
    }
   ],
   "source": [
    "def ner_contains_and_count_norm(candidate): \n",
    "    entities = nlp(str(candidate))\n",
    "    contains = 0\n",
    "    ner_norm = 0\n",
    "    \n",
    "    if len(entities) > 0:\n",
    "        contains = 1\n",
    "        ner_norm = len(entities) / len(candidate)\n",
    "    \n",
    "    return [contains, ner_norm]\n",
    "\n",
    "tqdm.pandas()  # <- added this for timer\n",
    "\n",
    "syn_candidate_queries['has_ne'], syn_candidate_queries['ne_norm'] = zip(*syn_candidate_queries[syn_candidate_queries.notnull()]['Synthetic_query'].progress_map(lambda candidate_row: ner_contains_and_count_norm(candidate_row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save intermediate syn_candidate_queries with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_candidate_queries.to_pickle(path_to_folder + 'created_sample/feature_vector_rough_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save feature_vector as CSV-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector.to_csv(path_to_folder + 'created_sample/feature_vector_rough_1m.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load intermediate syn_candidate_queries as feature_vector with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = pd.read_pickle(path_to_folder + 'created_sample/feature_vector_rough_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 'Other'-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_features(prefix, suffix, candidate, historical_logs):\n",
    "    # Boolean indicating whether the prefix ends with a space\n",
    "    bool_space = prefix.endswith(\" \")\n",
    "    \n",
    "    # The frequency of the candidate in the historical logs\n",
    "    frequency = historical_logs[candidate]\n",
    "    \n",
    "    # Prefix, suffix and total length in characters\n",
    "    prefixlen_char = len(prefix)\n",
    "    suffixlen_char = len(suffix)\n",
    "    totallen_char = len(candidate)\n",
    "    \n",
    "    # Prefix, suffix and total length in words\n",
    "    prefixlen_word = len(prefix.split())\n",
    "    suffixlen_word = len(suffix.split())\n",
    "    totallen_word = len(candidate.split())\n",
    "    \n",
    "    return [frequency, \n",
    "            prefixlen_char, suffixlen_char, totallen_char,\n",
    "            prefixlen_word, suffixlen_word, totallen_word,\n",
    "            bool_space * 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector['candid_freq'], feature_vector['prefixlen_char'], feature_vector['suffixlen_char'], feature_vector['totallen_char'], feature_vector['prefixlen_word'], feature_vector['suffixlen_word'], feature_vector['totallen_word'], feature_vector['bool_space'] = zip(*feature_vector.apply(lambda query_row: get_other_features(query_row.Prefix, query_row.Suffix, query_row.Synthetic_query, historical_dict), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create \"BERT\"-feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "\n",
    "# import mxnet as mx\n",
    "# import mxnet.ndarray as nd\n",
    "# from mxnet import nd, autograd, gluon\n",
    "# from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "# import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_BERT(prefix, suffix, bert_model):\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.simplefilter(\"ignore\")\n",
    "#         deserialized_net = gluon.nn.SymbolBlock.imports(path_to_folder + 'created_sample/trained_bert_classifier_10k-symbol.json', ['data'], path_to_folder + 'created_sample/trained_bert_classifier_10k-0003.params', ctx=ctx)\n",
    "\n",
    "\n",
    "#         verify_loaded_model(deserialized_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save feature_vector as CSV-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector.to_csv(path_to_folder + 'created_sample/feature_vector_1m.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save feature_vector as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector.to_pickle(path_to_folder + 'created_sample/feature_vector_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_feat_vec = feature_vector\n",
    "\n",
    "del cl_feat_vec['Query_clean']\n",
    "del cl_feat_vec['Prefix']\n",
    "del cl_feat_vec['Suffix']\n",
    "del cl_feat_vec['Synthetic_query']\n",
    "\n",
    "cl_feat_vec.to_csv(path_to_folder + 'created_sample/feature_vec_opt_1m.csv', index = False)\n",
    "cl_feat_vec.to_pickle(path_to_folder + 'created_sample/feature_vec_opt_1m.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:09:06.511441\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
