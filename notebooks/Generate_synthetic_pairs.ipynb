{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import datetime\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.util import ngrams\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from bisect import bisect_left\n",
    "\n",
    "path_to_folder = '/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features for dataset\n",
    "#df = pd.concat([pd.read_csv(f, delimiter='\\t') for f in glob.glob(path_to_folder + 'data/user-ct-test-collection-*.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled history dataset (for suffixes)\n",
    "This sample dataset is put on 1.000.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_hist = df.sample(1000000, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_hist.to_pickle(path_to_folder + 'created_sample/sample_hist_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load samples_hist with pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_hist = pd.read_pickle(path_to_folder + 'created_sample/sample_hist_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create suffixes\n",
    "Create all possible suffixes, splitting per word iteratively, removing leading words.\n",
    "\n",
    "Query \"How to cook CHICKEN\" becomes:\n",
    "- how to cook chicken\n",
    "- to cook chicken\n",
    "- cook chicken\n",
    "- chicken\n",
    "\n",
    "Creating 4 suffixes.\n",
    "\n",
    "All symbols are removed and changed to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suffixes = []\n",
    "#\n",
    "#for row in samples_hist.itertuples():\n",
    "#    line = re.sub(r\"[^A-Za-z0-9]+\", \" \", str(row.Query)).lower()\n",
    "#    words = line.split()\n",
    "#    for j in range(0, len(words)):\n",
    "#        suffix = \" \".join(words[j:])\n",
    "#        suffixes.append(suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/hist_suffixes.pickle_1m', 'wb') as f:\n",
    "#    pickle.dump(suffixes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load suffixes with pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path_to_folder + 'created_sample/hist_suffixes_1m.pickle', 'rb') as f:\n",
    "    suffixes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled dataset (for prefixes)\n",
    "This sample dataset is put on 10.000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_data = df.sample(10000, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples_data.to_pickle(path_to_folder + 'created_sample/sample_data_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load samples_data with pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_data = pd.read_pickle(path_to_folder + 'created_sample/sample_data_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create prefixes\n",
    "Create all possible prefixes, splitting per letter (from second word) iteratively, removing leading words.\n",
    "\n",
    "Query \"How to cook CHICKEN\" becomes (_ = space):\n",
    "- how\n",
    "- how_\n",
    "- how_t\n",
    "- ...\n",
    "- how_to_cook_chicken\n",
    "\n",
    "Creating 17 prefixes.\n",
    "\n",
    "All symbols are removed and changed to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_list = []\n",
    "\n",
    "candidate_list = []\n",
    "\n",
    "qid = 1\n",
    "\n",
    "for row in samples_data.itertuples():\n",
    "    line = re.sub(r\"[^A-Za-z0-9]+\", \" \", str(row.Query)).lower()\n",
    "    \n",
    "    # Ignore empty strings\n",
    "    if len(line.split()) > 1:   \n",
    "        firstword = line.split()[0]\n",
    "    \n",
    "        for j in range(len(firstword) + 2 , len(line)+1):\n",
    "            if (len(line[0:j].split())) <= 0:\n",
    "                print(line)\n",
    "                print('m' + str(line[0:j]) + 'm')\n",
    "            # temp list will be filled as [0]: 'clean_query', [1]: 'qid', [2]: 'prefix'\n",
    "            prefix = line[0:j]\n",
    "            temp_list = ['', '', '']\n",
    "            temp_list[0] = line\n",
    "            temp_list[1] = str(qid)\n",
    "            temp_list[2] = prefix\n",
    "            # Add to prefix query list\n",
    "            prefix_list.append(temp_list)\n",
    "            \n",
    "            allwords = line.split()\n",
    "            wordstoremove = prefix.split()[:-1]\n",
    "            \n",
    "            suffix = \" \".join([x for x in allwords if x not in wordstoremove])\n",
    "            prefix_without_endterm = \" \".join(wordstoremove)\n",
    "            \n",
    "            # Append also non-synthetic queries\n",
    "            candidate_list.append([line, qid, prefix, suffix, 1, prefix_without_endterm + \" \" + suffix, 1])\n",
    "            qid += 1\n",
    "    \n",
    "prefix_queries = pd.DataFrame.from_records(prefix_list)\n",
    "prefix_queries.columns = ['Query_clean', 'Qid', 'Prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prefix_queries.to_pickle(path_to_folder + 'data/prefixes_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load prefix_queries with pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_queries = pd.read_pickle(path_to_folder + 'created_sample/prefixes_1m.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create synthetic queries\n",
    "Combine end term of prefix with top 10 suffixes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix_dict = collections.Counter(suffixes)\n",
    "suffix_list = suffix_dict.most_common()\n",
    "suff_set_sorted = sorted([i[0] for i in suffix_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depending on your hardware it might take 30 min to 90 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it contains suffix\n",
    "def bisect_contains_check(suffix_list, prefix):\n",
    "    try:\n",
    "        return suffix_list[bisect_left(suffix_list, prefix)].startswith(prefix)\n",
    "    except IndexError:\n",
    "        return False\n",
    "\n",
    "# Returns the prefix keys\n",
    "def bisect_list_slice(suffix_list, prefix):\n",
    "    return suffix_list[bisect_left(suffix_list, prefix):\n",
    "         bisect_left(suffix_list, prefix[:-1] + chr(ord(prefix[-1])+1))]\n",
    "\n",
    "\n",
    "for row in tqdm(prefix_queries.itertuples(), total=prefix_queries.shape[0]):\n",
    "    words = row.Prefix.split()\n",
    "    endterm = words[-1]\n",
    "    no_endterm = \" \".join(words[:-1])\n",
    "    \n",
    "    if (bisect_contains_check(suff_set_sorted, endterm)):\n",
    "        temp_keys = bisect_list_slice(suff_set_sorted, endterm)\n",
    "        \n",
    "        temp_suffix_dict = Counter()\n",
    "        \n",
    "        for key in temp_keys:\n",
    "            temp_suffix_dict[key] = suffix_dict.get(key)\n",
    "            \n",
    "        temp_suffix_list = temp_suffix_dict.most_common()[:10]\n",
    "        \n",
    "        for j in temp_suffix_list: \n",
    "            # Last four will be filled as [5]: 'suffix', [6]: 'Hist_Suffix_freq', [7]: 'Synthetic_query' [8]: 'matching'\n",
    "            temp_list = [row.Query_clean, row.Qid, row.Prefix, '', '', '', '']\n",
    "            temp_list[3] = j[0]\n",
    "            temp_list[4] = j[1]\n",
    "            temp_list[5] = str(no_endterm + \" \" + str(j[0]))\n",
    "            temp_list[6] = 0\n",
    "            \n",
    "            if str(row.Query_clean) == str(no_endterm + \" \"+ str(j[0])):\n",
    "                temp_list[6] = 1\n",
    "            \n",
    "            # Add to synthetic query list\n",
    "            candidate_list.append(temp_list)\n",
    "    \n",
    "syn_candidate_queries = pd.DataFrame.from_records(candidate_list)\n",
    "syn_candidate_queries.columns = ['Query_clean', 'Qid', 'Prefix', 'Suffix', 'Hist_Suffix_freq', 'Synthetic_query', 'Synthetic_match']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_candidate_queries['Qid'].replace('qid:','',regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save syn_candidate_queries in CSV-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_candidate_queries.to_csv(path_to_folder + 'created_sample/syn_candid_queries_1m.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save syn_candidate_queries in pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_candidate_queries.to_pickle(path_to_folder + 'created_sample/syn_candidate_queries_1m.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#syn_candidate_queries = pd.read_pickle(path_to_folder + 'created_sample/syn_candidate_queries_1m.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
