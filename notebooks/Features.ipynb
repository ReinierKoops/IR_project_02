{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk.util import ngrams\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to combine all data files, since the data set is split among different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(f, delimiter='\\t') for f in glob.glob('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/data/user-ct-test-collection-*.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we randomly sample 1 million queries from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = df.sample(1000000, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.to_pickle('queries.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create all possible suffixes by splitting the queries in words and iteratively removing the leading word, during this process, we normalize URLs by using a regex that detects the dots in URLs and replaces them by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = []\n",
    "\n",
    "for row in queries.itertuples():\n",
    "    query = row.Query\n",
    "    if isinstance(query, str):\n",
    "        line = re.sub(r\"\\.(.)\", r\" \\1\", query)\n",
    "        words = line.split()\n",
    "        for j in range(0, len(words)):\n",
    "            suffix = \" \".join(words[j:])\n",
    "            suffixes.append(suffix)\n",
    "\n",
    "#np.save('pickles/historical_logs', suffixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this process takes quite a while, we store the results. We choose a compressed format since the array is very large and needs to be stored efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.GzipFile(\"/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/pickles/historical_logs_big.npy.gz\", \"w\")\n",
    "np.save(file=f, arr=suffixes)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we already have a save-file, we can skip the steps performed before and start from this point on by reading the file in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = gzip.GzipFile('/Users/rwkoops/PycharmProjects/IR_project/IR_project_02/pickles/historical_logs.npy.gz', \"r\")\n",
    "# suffixes = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary of all the candidates by using a counter from the collections library. We sort the values to analyze the top-n occuring candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_dict = collections.Counter(suffixes)\n",
    "historical_sorted = sorted(historical_dict, key=historical_dict.get, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below creates a dataframe ranking the top n-word suffixes in the historical logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ranking(sorted_history, top_n=10, max_word_length=3):\n",
    "\n",
    "    top_candidates = pd.DataFrame(index=range(1, top_n+1))\n",
    "    \n",
    "    indices = [1] * max_word_length\n",
    "    \n",
    "    for suffix in sorted_history:\n",
    "        length = len(suffix.split())\n",
    "        for i in range(1, max_word_length + 1):\n",
    "            if length == i and indices[i-1] <= top_n:\n",
    "                top_candidates.at[indices[i-1], \"Top \"+str(i)+\"-word suffixes\"] = suffix\n",
    "                indices[i-1] += 1\n",
    "    return top_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate, below we create the top 20 n-word rankings for n = 1 to n = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_candidates = create_ranking(historical_sorted, top_n=20, max_word_length=5)\n",
    "display(HTML(top_candidates.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the frequencies of the top n-word suffixes using word clouds. Below we visualize the top 20 4-word suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_wordcloud(dictionary, amount_words, n=10):\n",
    "    new_dict = {}\n",
    "    for (key, value) in dictionary.items():\n",
    "        if len(key.split()) == amount_words:\n",
    "            new_dict[key] = value\n",
    "    \n",
    "    \n",
    "    wordcloud = WordCloud(max_words=n).generate_from_frequencies(new_dict)\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "gen_wordcloud(historical_dict, 4, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next features we need are the n-gram frequencies, as they are called in the paper. Below the method for the generation of these features is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_freq(candidate, historical_logs, n=6):\n",
    "    words = candidate.split()\n",
    "    ngram_features = []\n",
    "    for i in range(1, n + 1):\n",
    "        ngramfreq_i = 0\n",
    "        ngrams_i = ngrams(words, i)\n",
    "        for word in ngrams_i:\n",
    "            freq_g = historical_logs[\" \".join(word)]\n",
    "            ngramfreq_i += freq_g\n",
    "        ngram_features.append(ngramfreq_i)\n",
    "    return ngram_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a candidate is \"place to live in the greek islands for americans\". Below we show the extraction of the ngram frequencies for up to 6-grams of this candidate. The number at index i is how often all of the (i+1)-grams occur in the historical logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = \"place to live in the greek islands for americans\"\n",
    "ngramfeatures = ngram_freq(candidate, historical_dict, n=6)\n",
    "ngramfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last features are:\n",
    "* The frequency of the candidate in the historical logs\n",
    "* The length of the prefix in chars and words\n",
    "* The length of the suffix in chars and words\n",
    "* The length of the candidate in chars and words\n",
    "* A binary feature indicating whether the prefix ends with a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_other_features(candidate, historical_logs):\n",
    "    # The prefix of the candidate\n",
    "    prefix = candidate[0]\n",
    "    \n",
    "    # Boolean indicating whether the prefix ends with a space\n",
    "    bool_space = prefix.endswith(\" \")\n",
    "    \n",
    "    # The suffix of the candidate\n",
    "    suffix = candidate[1]\n",
    "    \n",
    "    # The complete query\n",
    "    complete = \"\"\n",
    "    if bool_space:\n",
    "        complete = candidate[0] + candidate[1]\n",
    "    else:\n",
    "        complete = candidate[0] + \" \" + candidate[1]\n",
    "    \n",
    "    # The frequency of the candidate in the historical logs\n",
    "    frequency = historical_logs[complete]\n",
    "    \n",
    "    # Prefix, suffix and total length in characters\n",
    "    prefixlen_char = len(prefix)\n",
    "    suffixlen_char = len(suffix)\n",
    "    totallen_char = len(complete)\n",
    "    \n",
    "    # Prefix, suffix and total length in words\n",
    "    prefixlen_word = len(prefix.split())\n",
    "    suffixlen_word = len(suffix.split())\n",
    "    totallen_word = len(complete.split())\n",
    "    \n",
    "    return [frequency, \n",
    "            prefixlen_char, suffixlen_char, totallen_char,\n",
    "            prefixlen_word, suffixlen_word, totallen_word,\n",
    "            bool_space * 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of the extraction of the 'other features' from two example candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate1 = [\"How to\", \"cook an egg\"]\n",
    "candidate2 = [\"The best places \", \"to visit\"]\n",
    "\n",
    "f1 = get_other_features(candidate1, historical_dict)\n",
    "f2 = get_other_features(candidate2, historical_dict)\n",
    "print(f1)\n",
    "print(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
